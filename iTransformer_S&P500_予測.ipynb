{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGfB6W+hLLBqsoCNqkWw0i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ubDHb_WXdwq",
        "outputId": "1bc51a5e-9f19-4a92-cdd2-351d64e05f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uv\n",
            "  Downloading uv-0.7.19-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.7.19-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.4/18.4 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.7.19\n",
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 113ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m7 packages\u001b[0m \u001b[2min 200ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfredapi\u001b[0m\u001b[2m==0.5.2\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 94ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 83ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install uv\n",
        "!uv pip install tensorflow\n",
        "!uv pip install fredapi\n",
        "!uv pip install yfinance\n",
        "!uv pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from fredapi import Fred\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "4euLbFZ8XjmA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 設定とAPIキー ---\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# .env ファイルから環境変数をロード\n",
        "load_dotenv()\n",
        "\n",
        "# --- 設定とAPIキー ---\n",
        "# FRED APIキーを環境変数から取得\n",
        "fred_api_key = os.getenv('FRED_API_KEY')\n",
        "if fred_api_key is None:\n",
        "    raise ValueError(\"FRED_API_KEY が .env ファイルに設定されていません。\")\n",
        "\n",
        "fred = Fred(api_key=fred_api_key) # APIキーを初期化\n",
        "\n",
        "\n",
        "# データの開始日と終了日\n",
        "START_DATE = '2000-01-01'\n",
        "END_DATE = datetime.date.today().strftime('%Y-%m-%d') # 現在の日付まで"
      ],
      "metadata": {
        "id": "mWxSOFb7XnO8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fred_series_for_model = {\n",
        "    # 主要マクロ経済指標 (既存)\n",
        "    'Industrial Production': 'INDPRO',\n",
        "    'Unemployment Rate': 'UNRATE',\n",
        "    'CPI (All Urban)': 'CPIAUCSL',\n",
        "    'Federal Funds Rate': 'FEDFUNDS',\n",
        "    '10-Year Treasury Yield': 'DGS10',\n",
        "    '2-Year Treasury Yield': 'DGS2',\n",
        "    'VIX Index': 'VIXCLS',\n",
        "    'Oil Price WTI': 'DCOILWTICO',\n",
        "    'Consumer Sentiment': 'UMCSENT',\n",
        "    'M2 Money Stock': 'M2SL',\n",
        "    'Initial Claims': 'ICSA',\n",
        "    'Personal Consumption Expenditures': 'PCEC',\n",
        "    'Producer Price Index': 'PPIACO',\n",
        "    'Housing Starts': 'HOUST',\n",
        "\n",
        "    # 金融政策・金利指標 (既存)\n",
        "    '10-Year Breakeven Inflation Rate': 'T10YIE',\n",
        "    'Effective Federal Funds Rate': 'EFFR',\n",
        "    'Bank Prime Loan Rate': 'PRIME',\n",
        "    'TED Spread': 'TEDRATE',\n",
        "\n",
        "    # インフレ関連 (既存)\n",
        "    'PCE Price Index': 'PCEPI',\n",
        "    'PCE Price Index Excl. Food and Energy': 'PCEPILFE',\n",
        "\n",
        "    # 労働市場関連 (既存)\n",
        "    'All Employees, Total Nonfarm Payrolls': 'PAYEMS',\n",
        "    'Average Hourly Earnings': 'AHETPI',\n",
        "\n",
        "    # 住宅・不動産市場 (既存)\n",
        "    'New Home Sales': 'HSN1F',\n",
        "    'Existing Home Sales': 'EXHOSLUSM495S',\n",
        "    'S&P/Case-Shiller U.S. National HPI': 'CSUSHPINSA',\n",
        "\n",
        "    # 製造業・産業指標（既存 & 重複除去）\n",
        "    # 'Durable Goods Orders': 'DGORDER', # 後で再度追加されるため重複\n",
        "    'Capacity Utilization': 'TCU',\n",
        "    'Manufacturing Capacity Utilization': 'CUMFNS',\n",
        "    'Industrial Production: Manufacturing': 'IPMAN',\n",
        "    # 'New Orders: Durable Goods': 'DGORDER', # 後で再度追加されるため重複\n",
        "    'New Orders: Nondefense Capital Goods ex Aircraft': 'NEWORDER',\n",
        "    'Business Inventories': 'BUSINV',\n",
        "    'Inventory to Sales Ratio': 'ISRATIO',\n",
        "    'Chicago Fed National Activity Index': 'CFNAI',\n",
        "\n",
        "    # 為替・国際指標 (既存)\n",
        "    'Euro to U.S. Dollar Exchange Rate': 'DEXUSEU',\n",
        "    'Japanese Yen to U.S. Dollar Exchange Rate': 'DEXJPUS',\n",
        "    'Trade Weighted US Dollar Index: Broad': 'DTWEXBGS',\n",
        "\n",
        "    # コモディティ価格 (既存)\n",
        "    'Gold Price: London Fixing': 'IR14270',\n",
        "    'Copper Price': 'PCOPPUSDM',\n",
        "    'Brent Crude Oil Price': 'DCOILBRENTEU',\n",
        "\n",
        "    # GDP・所得・消費 (既存)\n",
        "    'Real Gross Domestic Product': 'GDPC1',\n",
        "    'Retail Sales': 'RSALES',\n",
        "    'Personal Income': 'PI',\n",
        "    'Retail Sales Excl Auto': 'RSXFS',\n",
        "\n",
        "    # 人口統計 (既存)\n",
        "    'Total US Population': 'POP',\n",
        "    'Civilian Noninstitutional Population': 'CNP16OV',\n",
        "    # 'Resident Population 16+': 'CNP16OV', # 上と重複\n",
        "\n",
        "    # 移民関連統計 (既存)\n",
        "    'Foreign-Born Labor Force': 'LNS11000006',\n",
        "    'Foreign Born Employment Level': 'LNS12032194',\n",
        "    'Foreign Born Population': 'LNU00073395',\n",
        "\n",
        "    # FRB政策金利上限 (既存)\n",
        "    'FRB Target Rate Upper Bound': 'DFEDTARU',\n",
        "\n",
        "    # --- ここから追加提案分 ---\n",
        "\n",
        "    # 市場センチメント/リスク指標\n",
        "    'AAII Investor Sentiment Survey (Bullish %)': 'AAII_BULL', # 週次\n",
        "    # 'Personal Saving Rate': 'PSAVERT', # 既存で含まれている\n",
        "\n",
        "    # 金融環境/信用状況\n",
        "    # 'Commercial and Industrial Loans': 'BUSLOANS', # 既存で含まれている\n",
        "    # 'Total Consumer Credit Outstanding': 'TOTALSL', # 既存で含まれている\n",
        "    'Monetary Base': 'BOGMBASE', # 週次\n",
        "    'St. Louis Fed Financial Stress Index': 'STLFSI', # 週次\n",
        "    'BAA Corporate Bond Yield minus 10-Year Treasury Yield': 'BAA10Y', # 日次\n",
        "\n",
        "    # 不動産市場の深掘り\n",
        "    # 'Housing Starts': 'HOUST', # 既存で含まれている\n",
        "    'Building Permits, Total': 'PERMIT', # 月次\n",
        "    '30-Year Fixed Rate Mortgage Average in the United States': 'MORTGAGE30US', # 週次\n",
        "\n",
        "    # 国際経済/貿易\n",
        "    'Real GDP for China': 'CPGDPAI', # 四半期\n",
        "    'Real GDP for Euro Area': 'EUGDPRQFN', # 四半期\n",
        "    'Exports of Goods and Services': 'BOPGSTB', # 月次\n",
        "    'Imports of Goods and Services': 'BOPIMPGSA', # 月次\n",
        "\n",
        "    # コモディティ価格の多様化\n",
        "    'Natural Gas Price (Henry Hub)': 'DHHNGSP', # 日次\n",
        "    'Silver Price: London Fixing': 'PMFIXAM', # 日次\n",
        "\n",
        "    # 労働市場の質/ダイナミクス\n",
        "    # 'Job Openings: Total Nonfarm': 'JTSJOL', # 既存で含まれている\n",
        "    # 'Labor Force Participation Rate': 'CIVPART', # 既存で含まれている\n",
        "    'Quits Level: Total Nonfarm': 'JTSQUL', # 月次\n",
        "\n",
        "    # 景気先行指標の多様化\n",
        "    # 'Leading Index for the United States': 'USSLIND', # 既存で含まれている\n",
        "    # 'NFIB Small Business Optimism Index': 'SBOITTL', # 既存で含まれている\n",
        "    'Consumer Confidence Index (CCI)': 'CSCICP01USM665S', # 月次\n",
        "\n",
        "    # その他\n",
        "    # 'Retail Sales Excl Auto': 'RSXFS', # 既存で含まれている\n",
        "    'Government Consumption Expenditures & Gross Investment': 'GCE', # 四半期\n",
        "\n",
        "    # 製造業関連の重複を整理し、新しいものを追加\n",
        "    'Durable Goods Orders': 'DGORDER', # 既存で一番上にあるため、ここに集約\n",
        "    'New Orders, Durable Goods excl Defense': 'ADXDNO',\n",
        "    'New Orders, Consumer Durable Goods': 'UCDGNO',\n",
        "    'Manufacturers Sales': 'MNFCTRSMSA',\n",
        "    'Total Manufacturing New Orders (SA)': 'AMTMNO', # 新規追加\n",
        "    'Total Manufacturing New Orders (NSA)': 'UMTMNO', # 新規追加\n",
        "}\n",
        "\n",
        "\n",
        "def get_fred_data(series_dict, start_date, end_date):\n",
        "    \"\"\"FREDから経済指標データを取得し、日次データに変換して結合する\"\"\"\n",
        "    df_list = []\n",
        "    print(\"FREDデータを取得中...\")\n",
        "    for name, series_id in series_dict.items():\n",
        "        try:\n",
        "            data = fred.get_series(series_id, start_date, end_date)\n",
        "            if data is not None and not data.empty:\n",
        "                data = data.rename(name).to_frame()\n",
        "                data.index = pd.to_datetime(data.index)\n",
        "                df_list.append(data)\n",
        "                print(f\"  - {name} ({series_id}) 取得完了。\")\n",
        "            else:\n",
        "                print(f\"  - 警告: {name} ({series_id}) のデータが取得できませんでした。\")\n",
        "        except Exception as e:\n",
        "            print(f\"  - エラー: {name} ({series_id}) の取得中にエラーが発生しました: {e}\")\n",
        "\n",
        "    if not df_list:\n",
        "        raise ValueError(\"FREDからデータを一つも取得できませんでした。APIキーとシリーズIDを確認してください。\")\n",
        "\n",
        "    # 全てのFREDデータを結合\n",
        "    fred_df = pd.concat(df_list, axis=1)\n",
        "    fred_df = fred_df.asfreq('D') # 日次頻度に変換\n",
        "    fred_df = fred_df.ffill() # 前の値で欠損値を埋める (経済指標は更新頻度が低いため)\n",
        "    fred_df = fred_df.bfill() # 残った欠損値は次の値で埋める (最初の方の欠損対策)\n",
        "    fred_df = fred_df.astype(np.float32) # メモリ効率のためfloat32に変換\n",
        "    print(\"FREDデータ取得と前処理完了。\")\n",
        "    return fred_df\n",
        "\n",
        "\n",
        "fred_data = get_fred_data(fred_series_for_model, START_DATE, END_DATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9tRnf4JX4nq",
        "outputId": "92849f82-9ee9-40a9-b856-088e36ad91cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FREDデータを取得中...\n",
            "  - Industrial Production (INDPRO) 取得完了。\n",
            "  - Unemployment Rate (UNRATE) 取得完了。\n",
            "  - CPI (All Urban) (CPIAUCSL) 取得完了。\n",
            "  - Federal Funds Rate (FEDFUNDS) 取得完了。\n",
            "  - 10-Year Treasury Yield (DGS10) 取得完了。\n",
            "  - 2-Year Treasury Yield (DGS2) 取得完了。\n",
            "  - VIX Index (VIXCLS) 取得完了。\n",
            "  - Oil Price WTI (DCOILWTICO) 取得完了。\n",
            "  - Consumer Sentiment (UMCSENT) 取得完了。\n",
            "  - M2 Money Stock (M2SL) 取得完了。\n",
            "  - Initial Claims (ICSA) 取得完了。\n",
            "  - Personal Consumption Expenditures (PCEC) 取得完了。\n",
            "  - Producer Price Index (PPIACO) 取得完了。\n",
            "  - Housing Starts (HOUST) 取得完了。\n",
            "  - 10-Year Breakeven Inflation Rate (T10YIE) 取得完了。\n",
            "  - Effective Federal Funds Rate (EFFR) 取得完了。\n",
            "  - Bank Prime Loan Rate (PRIME) 取得完了。\n",
            "  - TED Spread (TEDRATE) 取得完了。\n",
            "  - PCE Price Index (PCEPI) 取得完了。\n",
            "  - PCE Price Index Excl. Food and Energy (PCEPILFE) 取得完了。\n",
            "  - All Employees, Total Nonfarm Payrolls (PAYEMS) 取得完了。\n",
            "  - Average Hourly Earnings (AHETPI) 取得完了。\n",
            "  - New Home Sales (HSN1F) 取得完了。\n",
            "  - Existing Home Sales (EXHOSLUSM495S) 取得完了。\n",
            "  - S&P/Case-Shiller U.S. National HPI (CSUSHPINSA) 取得完了。\n",
            "  - Capacity Utilization (TCU) 取得完了。\n",
            "  - Manufacturing Capacity Utilization (CUMFNS) 取得完了。\n",
            "  - Industrial Production: Manufacturing (IPMAN) 取得完了。\n",
            "  - New Orders: Nondefense Capital Goods ex Aircraft (NEWORDER) 取得完了。\n",
            "  - Business Inventories (BUSINV) 取得完了。\n",
            "  - Inventory to Sales Ratio (ISRATIO) 取得完了。\n",
            "  - Chicago Fed National Activity Index (CFNAI) 取得完了。\n",
            "  - Euro to U.S. Dollar Exchange Rate (DEXUSEU) 取得完了。\n",
            "  - Japanese Yen to U.S. Dollar Exchange Rate (DEXJPUS) 取得完了。\n",
            "  - Trade Weighted US Dollar Index: Broad (DTWEXBGS) 取得完了。\n",
            "  - Gold Price: London Fixing (IR14270) 取得完了。\n",
            "  - Copper Price (PCOPPUSDM) 取得完了。\n",
            "  - Brent Crude Oil Price (DCOILBRENTEU) 取得完了。\n",
            "  - Real Gross Domestic Product (GDPC1) 取得完了。\n",
            "  - Retail Sales (RSALES) 取得完了。\n",
            "  - Personal Income (PI) 取得完了。\n",
            "  - Retail Sales Excl Auto (RSXFS) 取得完了。\n",
            "  - Total US Population (POP) 取得完了。\n",
            "  - Civilian Noninstitutional Population (CNP16OV) 取得完了。\n",
            "  - Foreign-Born Labor Force (LNS11000006) 取得完了。\n",
            "  - Foreign Born Employment Level (LNS12032194) 取得完了。\n",
            "  - Foreign Born Population (LNU00073395) 取得完了。\n",
            "  - FRB Target Rate Upper Bound (DFEDTARU) 取得完了。\n",
            "  - エラー: AAII Investor Sentiment Survey (Bullish %) (AAII_BULL) の取得中にエラーが発生しました: Bad Request.  Invalid value for variable series_id.  Series IDs should be 25 or less alphanumeric characters.\n",
            "  - Monetary Base (BOGMBASE) 取得完了。\n",
            "  - St. Louis Fed Financial Stress Index (STLFSI) 取得完了。\n",
            "  - BAA Corporate Bond Yield minus 10-Year Treasury Yield (BAA10Y) 取得完了。\n",
            "  - Building Permits, Total (PERMIT) 取得完了。\n",
            "  - 30-Year Fixed Rate Mortgage Average in the United States (MORTGAGE30US) 取得完了。\n",
            "  - Real GDP for China (CPGDPAI) 取得完了。\n",
            "  - エラー: Real GDP for Euro Area (EUGDPRQFN) の取得中にエラーが発生しました: Bad Request.  The series does not exist.\n",
            "  - Exports of Goods and Services (BOPGSTB) 取得完了。\n",
            "  - エラー: Imports of Goods and Services (BOPIMPGSA) の取得中にエラーが発生しました: Bad Request.  The series does not exist.\n",
            "  - Natural Gas Price (Henry Hub) (DHHNGSP) 取得完了。\n",
            "  - エラー: Silver Price: London Fixing (PMFIXAM) の取得中にエラーが発生しました: Bad Request.  The series does not exist.\n",
            "  - Quits Level: Total Nonfarm (JTSQUL) 取得完了。\n",
            "  - エラー: Consumer Confidence Index (CCI) (CSCICP01USM665S) の取得中にエラーが発生しました: Bad Request.  The series does not exist.\n",
            "  - Government Consumption Expenditures & Gross Investment (GCE) 取得完了。\n",
            "  - Durable Goods Orders (DGORDER) 取得完了。\n",
            "  - New Orders, Durable Goods excl Defense (ADXDNO) 取得完了。\n",
            "  - New Orders, Consumer Durable Goods (UCDGNO) 取得完了。\n",
            "  - Manufacturers Sales (MNFCTRSMSA) 取得完了。\n",
            "  - Total Manufacturing New Orders (SA) (AMTMNO) 取得完了。\n",
            "  - Total Manufacturing New Orders (NSA) (UMTMNO) 取得完了。\n",
            "FREDデータ取得と前処理完了。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Yahoo Financeから株価データを取得 ---\n",
        "yahoo_tickers_for_model = {\n",
        "    'S&P500': '^GSPC',  # S&P 500指数 (予測ターゲット)\n",
        "    'SOX Index': '^SOX',  # PHLX Semiconductor Sector Index\n",
        "\n",
        "    # 主要テクノロジー企業\n",
        "    'NVIDIA': 'NVDA',\n",
        "    'Amazon': 'AMZN',\n",
        "    'Apple': 'AAPL',\n",
        "    'TSM': 'TSM',        # Taiwan Semiconductor Manufacturing Company\n",
        "    'ASML': 'ASML',      # ASML Holding N.V.\n",
        "    'META': 'META',      # Meta Platforms, Inc.\n",
        "    'Microsoft': 'MSFT', # Microsoft Corporation\n",
        "    'Google': 'GOOGL',   # Alphabet Inc. (Class A)\n",
        "\n",
        "    # その他のセクターリーダー\n",
        "    'Tesla': 'TSLA',     # Tesla, Inc.\n",
        "    'JPMorgan': 'JPM',   # JPMorgan Chase & Co.\n",
        "    'Eli Lilly': 'LLY',  # Eli Lilly and Company\n",
        "    'ExxonMobil': 'XOM', # Exxon Mobil Corporation\n",
        "}\n",
        "\n",
        "yahoo_tickers_for_model.update({\n",
        "    # セクターリーダー追加\n",
        "    'Berkshire Hathaway': 'BRK-B',\n",
        "    'UnitedHealth': 'UNH',\n",
        "    'Procter & Gamble': 'PG',\n",
        "    'Johnson & Johnson': 'JNJ',\n",
        "    'Visa': 'V',\n",
        "    'Walmart': 'WMT',\n",
        "    'Caterpillar': 'CAT',\n",
        "    'Chevron': 'CVX',\n",
        "    'Boeing': 'BA',\n",
        "\n",
        "    # コモディティ・経済指標\n",
        "    'Gold': 'GC=F',\n",
        "    'Silver': 'SI=F',\n",
        "    'Crude Oil': 'CL=F',\n",
        "    'Brent Oil': 'BZ=F',\n",
        "\n",
        "    'USD/JPY': 'JPY=X',\n",
        "    'EUR/USD': 'EURUSD=X',\n",
        "\n",
        "    'US10Y Yield': '^TNX',\n",
        "    'US2Y Yield': '^IRX',\n",
        "    'TIP ETF': 'TIP',\n",
        "    'TLT ETF': 'TLT',\n",
        "    'SHY ETF': 'SHY',\n",
        "    'IEF ETF': 'IEF',\n",
        "\n",
        "    'VIX': '^VIX',\n",
        "    'DXY Index': 'DX-Y.NYB',\n",
        "    'Baltic Dry Index': '^BDI',\n",
        "\n",
        "    # 補助ETF\n",
        "    'SPY': 'SPY',\n",
        "    'QQQ': 'QQQ',\n",
        "    'XLK': 'XLK',\n",
        "    'XLF': 'XLF',\n",
        "    'XLE': 'XLE',\n",
        "})\n",
        "\n",
        "\n",
        "def get_yahoo_data(tickers_dict, start_date, end_date):\n",
        "    \"\"\"Yahoo Financeから株価データを取得し、日次データに変換して結合する\"\"\"\n",
        "    combined_close_data = pd.DataFrame()\n",
        "    print(\"Yahoo Financeデータを取得中...\")\n",
        "\n",
        "    for display_name, ticker_symbol in tickers_dict.items():\n",
        "        print(f\"    フェッチ中: {display_name} ({ticker_symbol})...\")\n",
        "        try:\n",
        "            # progress=False を追加し、auto_adjust=True を明示\n",
        "            single_ticker_data = yf.download(ticker_symbol, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
        "            if not single_ticker_data.empty and 'Close' in single_ticker_data.columns:\n",
        "                combined_close_data[display_name] = single_ticker_data['Close']\n",
        "                print(f\"    {display_name}: {len(single_ticker_data)} データポイント取得\")\n",
        "            else:\n",
        "                print(f\"    警告: ティッカー {ticker_symbol} のデータが見つからないか、'Close'カラムがありません。\")\n",
        "        except Exception as e:\n",
        "            # エラータイプも表示するように改善\n",
        "            print(f\"    ティッカー {ticker_symbol} のフェッチ中にエラー: {type(e).__name__}: {e}\")\n",
        "\n",
        "    if combined_close_data.empty:\n",
        "        raise ValueError(\"Yahoo Financeからデータを一つも取得できませんでした。ティッカーを確認してください。\")\n",
        "\n",
        "    # 全てのYahoo Financeデータを結合\n",
        "    yahoo_df = combined_close_data.asfreq('D') # 日次頻度に変換\n",
        "    yahoo_df = yahoo_df.ffill() # 前の値で欠損値を埋める (週末・祝日対策)\n",
        "    yahoo_df = yahoo_df.bfill() # 残った欠損値は次の値で埋める (最初の方の欠損対策)\n",
        "    yahoo_df = yahoo_df.astype(np.float32) # メモリ効率のためfloat32に変換\n",
        "    print(\"Yahoo Financeデータ取得と前処理完了。\")\n",
        "    return yahoo_df\n",
        "\n",
        "yahoo_data = get_yahoo_data(yahoo_tickers_for_model, START_DATE, END_DATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fprP_61MX8Ci",
        "outputId": "9835f8b2-4670-41e4-e5c5-c0d22be64441"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yahoo Financeデータを取得中...\n",
            "    フェッチ中: S&P500 (^GSPC)...\n",
            "    S&P500: 6413 データポイント取得\n",
            "    フェッチ中: SOX Index (^SOX)...\n",
            "    SOX Index: 6413 データポイント取得\n",
            "    フェッチ中: NVIDIA (NVDA)...\n",
            "    NVIDIA: 6413 データポイント取得\n",
            "    フェッチ中: Amazon (AMZN)...\n",
            "    Amazon: 6413 データポイント取得\n",
            "    フェッチ中: Apple (AAPL)...\n",
            "    Apple: 6413 データポイント取得\n",
            "    フェッチ中: TSM (TSM)...\n",
            "    TSM: 6413 データポイント取得\n",
            "    フェッチ中: ASML (ASML)...\n",
            "    ASML: 6413 データポイント取得\n",
            "    フェッチ中: META (META)...\n",
            "    META: 3299 データポイント取得\n",
            "    フェッチ中: Microsoft (MSFT)...\n",
            "    Microsoft: 6413 データポイント取得\n",
            "    フェッチ中: Google (GOOGL)...\n",
            "    Google: 5251 データポイント取得\n",
            "    フェッチ中: Tesla (TSLA)...\n",
            "    Tesla: 3776 データポイント取得\n",
            "    フェッチ中: JPMorgan (JPM)...\n",
            "    JPMorgan: 6413 データポイント取得\n",
            "    フェッチ中: Eli Lilly (LLY)...\n",
            "    Eli Lilly: 6413 データポイント取得\n",
            "    フェッチ中: ExxonMobil (XOM)...\n",
            "    ExxonMobil: 6413 データポイント取得\n",
            "    フェッチ中: Berkshire Hathaway (BRK-B)...\n",
            "    Berkshire Hathaway: 6413 データポイント取得\n",
            "    フェッチ中: UnitedHealth (UNH)...\n",
            "    UnitedHealth: 6413 データポイント取得\n",
            "    フェッチ中: Procter & Gamble (PG)...\n",
            "    Procter & Gamble: 6413 データポイント取得\n",
            "    フェッチ中: Johnson & Johnson (JNJ)...\n",
            "    Johnson & Johnson: 6413 データポイント取得\n",
            "    フェッチ中: Visa (V)...\n",
            "    Visa: 4350 データポイント取得\n",
            "    フェッチ中: Walmart (WMT)...\n",
            "    Walmart: 6413 データポイント取得\n",
            "    フェッチ中: Caterpillar (CAT)...\n",
            "    Caterpillar: 6413 データポイント取得\n",
            "    フェッチ中: Chevron (CVX)...\n",
            "    Chevron: 6413 データポイント取得\n",
            "    フェッチ中: Boeing (BA)...\n",
            "    Boeing: 6413 データポイント取得\n",
            "    フェッチ中: Gold (GC=F)...\n",
            "    Gold: 6231 データポイント取得\n",
            "    フェッチ中: Silver (SI=F)...\n",
            "    Silver: 6233 データポイント取得\n",
            "    フェッチ中: Crude Oil (CL=F)...\n",
            "    Crude Oil: 6240 データポイント取得\n",
            "    フェッチ中: Brent Oil (BZ=F)...\n",
            "    Brent Oil: 4458 データポイント取得\n",
            "    フェッチ中: USD/JPY (JPY=X)...\n",
            "    USD/JPY: 6621 データポイント取得\n",
            "    フェッチ中: EUR/USD (EURUSD=X)...\n",
            "    EUR/USD: 5601 データポイント取得\n",
            "    フェッチ中: US10Y Yield (^TNX)...\n",
            "    US10Y Yield: 6407 データポイント取得\n",
            "    フェッチ中: US2Y Yield (^IRX)...\n",
            "    US2Y Yield: 6407 データポイント取得\n",
            "    フェッチ中: TIP ETF (TIP)...\n",
            "    TIP ETF: 5427 データポイント取得\n",
            "    フェッチ中: TLT ETF (TLT)...\n",
            "    TLT ETF: 5769 データポイント取得\n",
            "    フェッチ中: SHY ETF (SHY)...\n",
            "    SHY ETF: 5769 データポイント取得\n",
            "    フェッチ中: IEF ETF (IEF)...\n",
            "    IEF ETF: 5769 データポイント取得\n",
            "    フェッチ中: VIX (^VIX)...\n",
            "    VIX: 6413 データポイント取得\n",
            "    フェッチ中: DXY Index (DX-Y.NYB)...\n",
            "    DXY Index: 6442 データポイント取得\n",
            "    フェッチ中: Baltic Dry Index (^BDI)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['^BDI']: YFTzMissingError('possibly delisted; no timezone found')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    警告: ティッカー ^BDI のデータが見つからないか、'Close'カラムがありません。\n",
            "    フェッチ中: SPY (SPY)...\n",
            "    SPY: 6413 データポイント取得\n",
            "    フェッチ中: QQQ (QQQ)...\n",
            "    QQQ: 6413 データポイント取得\n",
            "    フェッチ中: XLK (XLK)...\n",
            "    XLK: 6413 データポイント取得\n",
            "    フェッチ中: XLF (XLF)...\n",
            "    XLF: 6413 データポイント取得\n",
            "    フェッチ中: XLE (XLE)...\n",
            "    XLE: 6413 データポイント取得\n",
            "Yahoo Financeデータ取得と前処理完了。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- グローバル変数設定 ---\n",
        "WINDOW_SIZE = 252 #（約1年間の営業日 # 過去n日間のデータを使用\n",
        "# PRED_HORIZON = 1 # 1日後の予測 (元の設定)\n",
        "MAX_PRED_HORIZON = 30 # 1日から30日後までを予測\n",
        "\n",
        "TEST_SIZE_RATIO = 0.1 # テストデータの割合\n",
        "VALID_SIZE_RATIO = 0.1 # 検証データの割合\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100 # エポック数を増加させる可能性あり\n",
        "LEARNING_RATE = 1e-3"
      ],
      "metadata": {
        "id": "gHIhpFCeYBlb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. データ前処理と特徴量エンジニアリング ---\n",
        "def preprocess_data(fred_df, yahoo_df, window_size, max_pred_horizon):\n",
        "    \"\"\"\n",
        "    FREDとYahoo Financeのデータを結合し、特徴量エンジニアリングとターゲット生成を行う。\n",
        "    ターゲットはS&P 500の未来の複数日のリターンに備えるため、ここでは単一の未来リターンを計算。\n",
        "    シーケンス生成時に複数のリターンを抽出する。\n",
        "    \"\"\"\n",
        "    print(\"データ前処理と特徴量エンジニアリングを開始します。\")\n",
        "\n",
        "    # FREDとYahoo Financeデータを結合\n",
        "    df = pd.merge(yahoo_df, fred_df, left_index=True, right_index=True, how='left')\n",
        "    df = df.ffill()\n",
        "    df = df.bfill()\n",
        "    print(f\"結合後のデータ形状: {df.shape}\")\n",
        "    print(f\"結合後の欠損値数:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")\n",
        "\n",
        "    # S&P 500のターゲットを計算\n",
        "    # ここでは、将来の最も遠い予測点までデータがあることを保証するために、\n",
        "    # max_pred_horizon日後のリターンを計算しておき、その分だけ NaN を生成する。\n",
        "    # 実際の複数日ターゲットの抽出は prepare_sequences で行います。\n",
        "    df['SP500_Return'] = df['S&P500'].pct_change(1).shift(-1) * 100 # まず日次リターン*100を計算\n",
        "    # future_horizonまでのNaNを考慮してドロップ\n",
        "    df.dropna(subset=['SP500_Return'], inplace=True)\n",
        "    df = df.astype(np.float32)\n",
        "\n",
        "    # features_df を直接いじるのではなく、新しい特徴量を格納するリストを作成\n",
        "    new_features_list = []\n",
        "    # 元の特徴量もリストに含めるか、後で結合する\n",
        "    features_df_base = df.drop(columns=['SP500_Return', 'S&P500'], errors='ignore').copy()\n",
        "    new_features_list.append(features_df_base)\n",
        "\n",
        "    # 特徴量エンジニアリング\n",
        "    print(\"特徴量エンジニアリング中...\")\n",
        "\n",
        "    # 10Y-2Y スプレッド\n",
        "    if '10-Year Treasury Yield' in df.columns and '2-Year Treasury Yield' in df.columns:\n",
        "        spread = df['10-Year Treasury Yield'] - df['2-Year Treasury Yield']\n",
        "        new_features_list.append(spread.rename('10Y-2Y Spread'))\n",
        "\n",
        "    # 変化率 (Rate of Change - ROC)\n",
        "    roc_targets = list(yahoo_tickers_for_model.keys())[1:] + \\\n",
        "                  ['Industrial Production', 'Unemployment Rate', 'CPI (All Urban)', 'VIX Index', 'Oil Price WTI']\n",
        "    roc_targets = [col for col in roc_targets if col in df.columns] # 存在する列のみ\n",
        "    for col in roc_targets:\n",
        "        for p in [5, 20, 60]:\n",
        "            roc_col = df[col].pct_change(p) * 100\n",
        "            new_features_list.append(roc_col.rename(f'{col}_ROC_{p}D'))\n",
        "\n",
        "    # ラグ特徴量\n",
        "    lag_targets = ['SP500_Return', 'Industrial Production', 'Unemployment Rate', 'CPI (All Urban)',\n",
        "                   'VIX Index', 'Oil Price WTI'] + list(yahoo_tickers_for_model.keys())[1:]\n",
        "    lag_targets = [col for col in lag_targets if col in df.columns]\n",
        "    for col in lag_targets:\n",
        "        for p in [1, 5, 20, 60]:\n",
        "            lag_col = df[col].shift(p)\n",
        "            new_features_list.append(lag_col.rename(f'{col}_Lag_{p}D'))\n",
        "\n",
        "    # 移動平均 (Moving Averages)\n",
        "    ma_targets = list(yahoo_tickers_for_model.keys())[1:] + \\\n",
        "                 ['VIX Index', 'Oil Price WTI', 'Gold Price: London Fixing',\n",
        "                  '10-Year Treasury Yield', 'Federal Funds Rate']\n",
        "    ma_targets = [col for col in ma_targets if col in df.columns]\n",
        "    for col in ma_targets:\n",
        "        for p in [5, 20, 60, 120]:\n",
        "            ma_col = df[col].rolling(window=p).mean()\n",
        "            new_features_list.append(ma_col.rename(f'{col}_MA_{p}D'))\n",
        "\n",
        "    # 移動標準偏差 (Rolling Standard Deviation - ボラティリティ)\n",
        "    std_targets = ['SP500_Return', 'VIX Index', 'Oil Price WTI', 'Gold Price: London Fixing']\n",
        "    std_targets = [col for col in std_targets if col in df.columns]\n",
        "    for col in std_targets:\n",
        "        for p in [5, 20, 60]:\n",
        "            std_col = df[col].rolling(window=p).std()\n",
        "            new_features_list.append(std_col.rename(f'{col}_Std_{p}D'))\n",
        "\n",
        "    # 比率特徴量\n",
        "    if 'Gold Price: London Fixing' in df.columns and 'Oil Price WTI' in df.columns:\n",
        "        ratio = df['Gold Price: London Fixing'] / df['Oil Price WTI']\n",
        "        new_features_list.append(ratio.rename('Gold_Oil_Ratio'))\n",
        "    if 'CPI (All Urban)' in df.columns and 'Producer Price Index' in df.columns:\n",
        "        ratio = df['CPI (All Urban)'] / df['Producer Price Index']\n",
        "        new_features_list.append(ratio.rename('CPI_PPI_Ratio'))\n",
        "\n",
        "    # 全ての新しい特徴量を一度に結合\n",
        "    features_df = pd.concat(new_features_list, axis=1)\n",
        "\n",
        "    # ここでS&P500のカラムはドロップ（既にリターンに変換済みのため、features_df_baseで既に除外されているはず）\n",
        "    # 念のため、残っていればドロップ\n",
        "    features_df.drop(columns=['S&P500'], inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "    # 特徴量エンジニアリングによって生成されたNaNを削除 (初期のウィンドウ期間)\n",
        "    features_df_before_dropna_rows = features_df.shape[0] # dropna前の行数\n",
        "    features_df.dropna(inplace=True)\n",
        "    features_df_after_dropna_rows = features_df.shape[0] # dropna後の行数\n",
        "\n",
        "    deleted_rows_count = features_df_before_dropna_rows - features_df_after_dropna_rows\n",
        "\n",
        "    if deleted_rows_count > 0:\n",
        "        original_combined_rows = df.shape[0] # 結合後の元の行数 (SP500_Returnでdropnaした後の行数)\n",
        "        percentage_deleted = (deleted_rows_count / original_combined_rows) * 100\n",
        "        print(f\"特徴量エンジニアリングによって生成された {deleted_rows_count} 行の欠損値を含むデータを削除しました。\")\n",
        "        print(f\"これは元の結合データ ({original_combined_rows} 行) の {percentage_deleted:.2f}% にあたります。\")\n",
        "\n",
        "\n",
        "    # 特徴量とターゲットを結合\n",
        "    final_df = pd.merge(features_df, df['SP500_Return'], left_index=True, right_index=True, how='inner')\n",
        "\n",
        "    print(f\"最終的なデータ形状 (特徴量とターゲット): {final_df.shape}\")\n",
        "    print(f\"最終的なデータセットの欠損値数:\\n{final_df.isnull().sum()[final_df.isnull().sum() > 0]}\")\n",
        "    if final_df.isnull().sum().sum() > 0:\n",
        "        raise ValueError(\"特徴量エンジニアリング後に欠損値が残っています。データ処理を確認してください。\")\n",
        "\n",
        "    print(\"データ前処理と特徴量エンジニアリング完了。\")\n",
        "    return final_df\n",
        "\n",
        "# データを結合し、特徴量エンジニアリング\n",
        "# ここでS&P500の未来リターンがターゲットとして含まれる\n",
        "full_data_df = preprocess_data(fred_data, yahoo_data, WINDOW_SIZE, MAX_PRED_HORIZON)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3ja6rwvYCVY",
        "outputId": "407c39b3-3f4d-4c5b-d39d-b13cc86285de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データ前処理と特徴量エンジニアリングを開始します。\n",
            "結合後のデータ形状: (9313, 106)\n",
            "結合後の欠損値数:\n",
            "Series([], dtype: int64)\n",
            "特徴量エンジニアリング中...\n",
            "特徴量エンジニアリングによって生成された 119 行の欠損値を含むデータを削除しました。\n",
            "これは元の結合データ (9312 行) の 1.28% にあたります。\n",
            "最終的なデータ形状 (特徴量とターゲット): (9193, 631)\n",
            "最終的なデータセットの欠損値数:\n",
            "Series([], dtype: int64)\n",
            "データ前処理と特徴量エンジニアリング完了。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 4. データをシーケンスに分割\n",
        "\n",
        "def prepare_sequences(features_array, targets_array, dates_series, window_size, max_pred_horizon):\n",
        "    \"\"\"\n",
        "    特徴量配列とターゲット配列、対応する日付シリーズをモデル入力用のシーケンスデータに変換する。\n",
        "    1日から max_pred_horizon 日後までの日次リターンを予測するように変更。\n",
        "    Args:\n",
        "        features_array (np.array): スケーリング後の特徴量配列 (例: X_train_scaled)\n",
        "        targets_array (np.array): ターゲット値の配列 (例: y_train)\n",
        "        dates_series (pd.Series or np.array): 対応する日付のシリーズ/配列\n",
        "        window_size (int): 過去何日間のデータを使用するか (シーケンス長)\n",
        "        max_pred_horizon (int): 最大で何日後までのターゲットを予測するか (例: 30)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_seq, y_seq, dates_seq)\n",
        "            X_seq (np.array): シーケンス化された特徴量データ\n",
        "            y_seq (np.array): シーケンスに対応するターゲット値 (形状: [N, max_pred_horizon])\n",
        "            dates_seq (np.array): シーケンスに対応するターゲットの最終日付\n",
        "    \"\"\"\n",
        "    X, y, dates = [], [], []\n",
        "\n",
        "    # max_pred_horizon を考慮してループ範囲を調整\n",
        "    # 特徴量配列 (features_array) とターゲット配列 (targets_array) の長さは同じである必要があります。\n",
        "    # ターゲット配列の長さから、将来のmax_pred_horizon分のデータが必要なため、その分短くループ\n",
        "    # また、window_size分の過去データも必要なので、その分も考慮\n",
        "    for i in range(len(features_array) - window_size - max_pred_horizon + 1):\n",
        "        X.append(features_array[i:(i + window_size)])\n",
        "\n",
        "        # ここが変更点: window_sizeの直後から max_pred_horizon 日後までのリターンをリストに追加\n",
        "        # targets_array は既に preprocess_data で複数ホライズンターゲットが生成されている場合\n",
        "        # y_train, y_valid, y_test は単一カラムのままであることに注意\n",
        "        # prepare_sequences の y_seq 形状は [N, max_pred_horizon] とあるが、これは\n",
        "        # preprocess_data で生成されるターゲットが一つ(SP500_Return_H1D)の場合、\n",
        "        # targets_array が (N,) の形状になるため、ここで `target_returns_slice` は (max_pred_horizon,) となる\n",
        "        # もし、preprocess_data で複数ターゲット (H1D, H5D, H20D) が生成されている場合、\n",
        "        # targets_array 自体が既に (N, num_target_horizons) 形状になる。\n",
        "        # その場合、y_seqの形状は [N_sequences, num_target_horizons * max_pred_horizon] のようになるか、\n",
        "        # あるいは、y_seq = [[target_H1D_t+1, target_H5D_t+1, ...], [target_H1D_t+2, target_H5D_t+2, ...], ...]\n",
        "        # のように、各タイムステップの複数ターゲットをまとめる設計が必要。\n",
        "        # 現在の prepare_sequences は `targets_array` が単一のリターンカラムであることを前提としている。\n",
        "\n",
        "        # 既存の y_seq の形状コメント `[N, max_pred_horizon]` に合わせるには、\n",
        "        # targets_array が単一の `SP500_Return_H1D` のような日次リターンであることを想定し、\n",
        "        # その `max_pred_horizon` 個の未来の値を取る形とする。\n",
        "        target_returns_slice = targets_array[i + window_size : i + window_size + max_pred_horizon]\n",
        "\n",
        "        # もし target_returns_slice の長さが max_pred_horizon 未満なら、そのシーケンスはスキップ\n",
        "        # これは主にデータの終わり近くで発生する可能性があります。\n",
        "        if len(target_returns_slice) == max_pred_horizon:\n",
        "            y.append(target_returns_slice)\n",
        "            # 日付は最後のターゲットに対応する日付とする (予測の終わり)\n",
        "            dates.append(dates_series[i + window_size + max_pred_horizon - 1])\n",
        "        else:\n",
        "            # ログ出力などでスキップされたことを示すことも可能\n",
        "            # print(f\"警告: シーケンス {i} のターゲットが短いためスキップされました。現在の長さ: {len(target_returns_slice)}, 必要: {max_pred_horizon}\")\n",
        "            pass\n",
        "\n",
        "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32), np.array(dates)\n",
        "\n",
        "# tf.data.Datasetを生成するヘルパー関数 (変更なし)\n",
        "def create_tf_dataset(X_seq, y_seq, batch_size, shuffle=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_seq, y_seq))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X_seq))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# --- 6. データの取得と前処理の実行 ---\n",
        "# full_data_df は preprocess_data() によって生成済みと仮定\n",
        "# preprocess_data() は新しい引数 (target_scaling_factor, target_horizons) を持つバージョンを使用している前提\n",
        "\n",
        "# PCA適用前の特徴量次元数\n",
        "# full_data_df からターゲットカラムを除外して特徴量次元数を取得\n",
        "# current_target_columns_in_df は preprocess_data の呼び出し後に定義されていると仮定\n",
        "# 例: current_target_columns_in_df = [col for col in full_data_df.columns if col.startswith('SP500_Return_H')]\n",
        "current_target_columns_in_df = ['SP500_Return']\n",
        "\n",
        "original_feature_dim = full_data_df.drop(columns=current_target_columns_in_df, errors='ignore').shape[1]\n",
        "print(f\"特徴量次元数 (PCA適用なし): {original_feature_dim}\")\n",
        "\n",
        "\n",
        "# データを訓練・検証・テストに分割 (時系列順)\n",
        "train_size = int(len(full_data_df) * (1 - TEST_SIZE_RATIO - VALID_SIZE_RATIO))\n",
        "valid_size = int(len(full_data_df) * VALID_SIZE_RATIO)\n",
        "test_size = len(full_data_df) - train_size - valid_size\n",
        "\n",
        "train_df = full_data_df.iloc[:train_size].copy()\n",
        "valid_df = full_data_df.iloc[train_size:train_size + valid_size].copy()\n",
        "test_df = full_data_df.iloc[train_size + valid_size:].copy()\n",
        "\n",
        "print(f\"データ分割: 訓練={len(train_df)} 検証={len(valid_df)} テスト={len(test_df)}\")\n",
        "\n",
        "# 特徴量のスケーリング (訓練データに基づいて適合)\n",
        "# full_data_df からターゲットカラムを除外したものを特徴量として選択\n",
        "feature_cols = [col for col in full_data_df.columns if col not in current_target_columns_in_df]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(train_df[feature_cols])\n",
        "X_valid_scaled = scaler.transform(valid_df[feature_cols])\n",
        "X_test_scaled = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "# PCAは適用しないため、PCA関連のコードを削除\n",
        "\n",
        "# ターゲット変数を取得\n",
        "# preprocess_dataで複数ターゲットが生成されている場合、y_trainなどは複数カラムになる\n",
        "# prepare_sequences が `targets_array` を (N,) として扱うことを前提としているため、\n",
        "# ここでは一旦、最も短期のターゲット (H1D) のみを抽出する形にする。\n",
        "# もし複数のターゲットを prepare_sequences の y_seq に含めたい場合は、prepare_sequences のロジックも変更が必要。\n",
        "# ここでは、MAX_PRED_HORIZON は future prediction steps (t+1, t+2, ... t+MAX_PRED_HORIZON) を指す。\n",
        "# preprocess_dataで生成された各 horizon のターゲットを個別に扱うか、\n",
        "# それとも prepare_sequences の y_seq に含めるかを明確にする必要がある。\n",
        "\n",
        "# 現行の prepare_sequences の y_seq 形状は [N, max_pred_horizon] となっているので、\n",
        "# targets_array は単一の時系列データ (例: 'SP500_Return_H1D') であると解釈します。\n",
        "# もし `target_horizons=[1, 5, 20]` と preprocess_data で指定している場合、\n",
        "# ここでどのターゲットを `y_train` などに指定するかを決定する必要があります。\n",
        "# 通常、最も短期の予測 (例: 'SP500_Return_H1D') をベースとして使用し、\n",
        "# prepare_sequences がその未来の `MAX_PRED_HORIZON` ステップ分のデータを収集するようにします。\n",
        "# あるいは、prepare_sequences を修正して、preprocess_data で生成された全てのターゲットカラムを\n",
        "# 最終的な y_seq の形状 ([N_sequences, MAX_PRED_HORIZON, num_target_horizons])\n",
        "# のようにできるようにすることも考えられますが、複雑になるため、ここでは一旦H1Dをベースとします。\n",
        "\n",
        "# preprocess_data で生成されたターゲットカラムリストから、最も短期のものを選択\n",
        "if current_target_columns_in_df:\n",
        "    # ターゲットカラム名をソートして、一番若い日数のものを選ぶ (例: H1D)\n",
        "    primary_target_column = sorted(current_target_columns_in_df)[0]\n",
        "    print(f\"プライマリーターゲットカラムとして '{primary_target_column}' を使用します。\")\n",
        "else:\n",
        "    raise ValueError(\"ターゲットカラムが見つかりません。preprocess_dataの実行を確認してください。\")\n",
        "\n",
        "\n",
        "y_train = train_df[primary_target_column].values\n",
        "y_valid = valid_df[primary_target_column].values\n",
        "y_test = test_df[primary_target_column].values\n",
        "\n",
        "# 各セットの日付インデックスを取得\n",
        "dates_train_full = train_df.index\n",
        "dates_valid_full = valid_df.index\n",
        "dates_test_full = test_df.index\n",
        "\n",
        "# シーケンスデータの準備 (MAX_PRED_HORIZON を使用)\n",
        "print(\"シーケンスデータを準備中...\")\n",
        "X_train_seq, y_train_seq, dates_train = prepare_sequences(\n",
        "    X_train_scaled, y_train, dates_train_full, WINDOW_SIZE, MAX_PRED_HORIZON\n",
        ")\n",
        "X_valid_seq, y_valid_seq, dates_valid = prepare_sequences(\n",
        "    X_valid_scaled, y_valid, dates_valid_full, WINDOW_SIZE, MAX_PRED_HORIZON\n",
        ")\n",
        "X_test_seq, y_test_seq, dates_test = prepare_sequences(\n",
        "    X_test_scaled, y_test, dates_test_full, WINDOW_SIZE, MAX_PRED_HORIZON\n",
        ")\n",
        "\n",
        "print(f\"訓練シーケンス形状: X={X_train_seq.shape}, y={y_train_seq.shape}\")\n",
        "print(f\"検証シーケンス形状: X={X_valid_seq.shape}, y={y_valid_seq.shape}\")\n",
        "print(f\"テストシーケンス形状: X={X_test_seq.shape}, y={y_test_seq.shape}\")\n",
        "\n",
        "# tf.data.Datasetを作成\n",
        "train_dataset = create_tf_dataset(X_train_seq, y_train_seq, BATCH_SIZE, shuffle=False)\n",
        "valid_dataset = create_tf_dataset(X_valid_seq, y_valid_seq, BATCH_SIZE, shuffle=False)\n",
        "test_dataset = create_tf_dataset(X_test_seq, y_test_seq, BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdOPLs7-YJHO",
        "outputId": "bcbfd89a-bc52-40f3-dc53-552e7efef55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "特徴量次元数 (PCA適用なし): 630\n",
            "データ分割: 訓練=7354 検証=919 テスト=920\n",
            "プライマリーターゲットカラムとして 'SP500_Return' を使用します。\n",
            "シーケンスデータを準備中...\n",
            "訓練シーケンス形状: X=(7073, 252, 630), y=(7073, 30)\n",
            "検証シーケンス形状: X=(638, 252, 630), y=(638, 30)\n",
            "テストシーケンス形状: X=(639, 252, 630), y=(639, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iTransformerBlock クラスの定義\n",
        "# Transformerモデルに、self-Attention機構を入れることで、iTransfomerに\n",
        "class iTransformerBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    iTransformerモデルの基本ブロックを定義します。\n",
        "    標準的なTransformerブロックのSelf-Attention機構を、\n",
        "    「時間軸を特徴量、特徴量次元をトークン」として転置して適用します。\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout_rate, time_steps, **kwargs):\n",
        "        \"\"\"\n",
        "        iTransformerBlockの初期化\n",
        "        Args:\n",
        "            d_model (int): Attention層の内部次元。特徴量（変数）がこの次元に埋め込まれる。\n",
        "            num_heads (int): Multi-Head Attentionのヘッド数。\n",
        "            dropout_rate (float): ドロップアウト率。\n",
        "            time_steps (int): 入力シーケンスの時系列長 (T)。FFNの出力次元として使用。\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.time_steps = time_steps # T (WINDOW_SIZE)\n",
        "\n",
        "        # Multi-Head Self-Attention層の定義\n",
        "        # key_dim=d_model: 各ヘッドのキー/クエリ/バリューの次元（特徴量トークンの埋め込み次元）\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.attn_dropout = Dropout(dropout_rate) # Attention後のドロップアウト\n",
        "        self.attn_norm = LayerNormalization(epsilon=1e-6) # Attention後のLayerNormalization\n",
        "\n",
        "        # Feed-Forward Network (FFN) の深層化\n",
        "        # iTransformer論文の解釈に基づき、FFNは時系列特徴量（T次元）を変換する。\n",
        "        # 深層化のために中間層を追加。\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(d_model * 4, activation='relu'), # 最初のDense層 (活性化関数ReLU)\n",
        "            Dense(d_model * 2, activation='relu'), # <<== 追加：FFNの中間層（深層化のため）\n",
        "            Dense(time_steps) # 最終的なDense層（出力次元は時系列長Tのまま）\n",
        "        ])\n",
        "        self.ffn_dropout = Dropout(dropout_rate) # FFN後のドロップアウト\n",
        "        self.ffn_norm = LayerNormalization(epsilon=1e-6) # FFN後のLayerNormalization\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        iTransformerBlockのフォワードパス\n",
        "        Args:\n",
        "            inputs (tf.Tensor): 入力テンソル。形状は (batch_size, T, D_emb)。\n",
        "                                D_embは特徴量埋め込み後の次元（通常d_model）。\n",
        "            training (bool): 訓練モードかどうかを示すフラグ。ドロップアウトの挙動に影響。\n",
        "        Returns:\n",
        "            tf.Tensor: 処理後の出力テンソル。形状は入力と同じ (batch_size, T, D_emb)。\n",
        "        \"\"\"\n",
        "        # iTransformerの核心: 変数軸と時間軸を転置\n",
        "        # 入力: (batch_size, 時系列長T, 埋め込み特徴量D_emb)\n",
        "        # 転置後: (batch_size, 埋め込み特徴量D_emb, 時系列長T)\n",
        "        # これにより、Attentionが「D_emb個のトークン（変数）、各トークンはT次元の特徴量を持つ」として機能。\n",
        "        inputs_transposed = tf.transpose(inputs, perm=[0, 2, 1])\n",
        "\n",
        "        # Multi-Head Self-Attentionの適用\n",
        "        # クエリ(Q), キー(K), バリュー(V)は全て転置された入力。\n",
        "        # Attentionは埋め込み特徴量D_emb軸（トークン軸）に対して行われる。\n",
        "        attn_output = self.attention(inputs_transposed, inputs_transposed)\n",
        "        attn_output = self.attn_dropout(attn_output, training=training)\n",
        "        # 残差接続 (Residual connection) と層正規化 (LayerNormalization)\n",
        "        attn_output = self.attn_norm(inputs_transposed + attn_output) # (batch_size, D_emb, T)\n",
        "\n",
        "        # Feed-Forward Network (FFN) の適用\n",
        "        # FFNは、Attention後の各トークン（各変数）のT次元の特徴ベクトルを変換する。\n",
        "        ffn_output = self.ffn(attn_output) # (batch_size, D_emb, T)\n",
        "        ffn_output = self.ffn_dropout(ffn_output, training=training)\n",
        "        # 残差接続と層正規化\n",
        "        output = self.ffn_norm(attn_output + ffn_output) # (batch_size, D_emb, T)\n",
        "\n",
        "        # 最終出力を元の形状 (batch_size, T, D_emb) に転置し直す\n",
        "        return tf.transpose(output, perm=[0, 2, 1])\n",
        "\n",
        "\n",
        "def build_itransformer_model(time_steps, feature_dim, d_model, num_heads, num_blocks, dropout_rate, output_horizon):\n",
        "    \"\"\"\n",
        "    iTransformerモデルを構築する。\n",
        "    Args:\n",
        "        time_steps (int): シーケンス長 (T)\n",
        "        feature_dim (int): 特徴量の次元数 (D)。PCA適用前。\n",
        "        d_model (int): Attention層の内部次元\n",
        "        num_heads (int): Multi-Head Attentionのヘッド数\n",
        "        num_blocks (int): iTransformerブロックの数\n",
        "        dropout_rate (float): ドロップアウト率\n",
        "        output_horizon (int): モデルの出力次元数 (例: 1日後から30日後までの予測なので 30)\n",
        "    Returns:\n",
        "        tf.keras.Model: 構築されたiTransformerモデル\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=(time_steps, feature_dim), dtype=tf.float32)\n",
        "    x = inputs\n",
        "\n",
        "    # Feature embedding (各変数を d_model に線形変換) の深層化\n",
        "    # 各時間ステップの各特徴量に対して適用されるようにTimeDistributedを使用\n",
        "    x = tf.keras.layers.TimeDistributed(Dense(d_model, activation='relu'))(x) # <<== 活性化関数を追加 + 1層目\n",
        "    x = tf.keras.layers.TimeDistributed(Dense(d_model))(x) # <<== 追加：2層目の埋め込み層\n",
        "\n",
        "    # num_blocks が外から渡されるため、これで制御\n",
        "    for _ in range(num_blocks): # <<== NUM_BLOCKS の値で深さが変わる\n",
        "        x = iTransformerBlock(d_model=d_model, num_heads=num_heads,\n",
        "                              dropout_rate=dropout_rate, time_steps=time_steps)(x)\n",
        "\n",
        "    # Global Average Pooling (時間軸T方向に平均を取る)\n",
        "    # (batch_size, T, d_model) -> (batch_size, d_model)\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # 最終的な予測層: 出力次元を output_horizon に変更\n",
        "    # ここも深層化するためにDense層を追加\n",
        "    x = Dense(d_model // 2, activation='relu')(x) # <<== 追加：予測ヘッドの中間層\n",
        "    outputs = Dense(output_horizon)(x) # 1日後から MAX_PRED_HORIZON 日後までのリターンを予測\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 7. モデルの構築と訓練 ---\n",
        "print(\"--- モデル構築と訓練 ---\")\n",
        "\n",
        "# PCAを適用しないため、original_feature_dim をそのまま使用\n",
        "# original_feature_dim は 'データのシーケンス分割の変更' セクションで既に計算されている\n",
        "# feature_dim_after_pca の代わりに original_feature_dim を使用\n",
        "# feature_dim_after_pca = pca.n_components_ # この行は不要になる\n",
        "\n",
        "# iTransformerモデルのハイパーパラメータ\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "NUM_BLOCKS = 2\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "model = build_itransformer_model(\n",
        "    time_steps=WINDOW_SIZE,\n",
        "    feature_dim=original_feature_dim, # ★ ここを original_feature_dim に変更 ★\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_blocks=NUM_BLOCKS,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    output_horizon=MAX_PRED_HORIZON # ここを新しいパラメータに変更\n",
        ")\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0), loss='mse')\n",
        "model.summary()\n",
        "\n",
        "# コールバックの設定\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=12, min_lr=5e-8, verbose=1)\n",
        "\n",
        "print(\"モデル訓練を開始します...\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"モデル訓練完了。\")"
      ],
      "metadata": {
        "id": "oQE2uCYQYWTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "print(\"--- モデル評価 ---\")\n",
        "train_loss = model.evaluate(train_dataset, verbose=0)\n",
        "valid_loss = model.evaluate(valid_dataset, verbose=0)\n",
        "test_loss = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "print(f\"訓練セットのMSE: {train_loss:.4f}\")\n",
        "print(f\"検証セットのMSE: {valid_loss:.4f}\")\n",
        "print(f\"テストセットのMSE: {test_loss:.4f}\")\n",
        "\n",
        "# 予測の実行 (predsの形状が (N, MAX_PRED_HORIZON) になるため、.flatten() は削除)\n",
        "train_preds = model.predict(train_dataset)\n",
        "valid_preds = model.predict(valid_dataset)\n",
        "test_preds = model.predict(test_dataset)"
      ],
      "metadata": {
        "id": "DtsOY_RAZR9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果の可視化\n",
        "def plot_results_multi_horizon(history, y_true_seq, y_pred_seq, dates, set_name, pred_horizon_to_plot=None):\n",
        "    \"\"\"\n",
        "    訓練履歴と予測結果をプロットする関数（多出力対応版）\n",
        "    Args:\n",
        "        pred_horizon_to_plot (int or list of int, optional): プロットしたい予測ホライズン。\n",
        "                                                             Noneの場合、1日後の予測のみプロット。\n",
        "    \"\"\"\n",
        "    if pred_horizon_to_plot is None:\n",
        "        plot_horizons = [0] # 0-indexed で 1日後の予測\n",
        "    elif isinstance(pred_horizon_to_plot, int):\n",
        "        plot_horizons = [pred_horizon_to_plot - 1] # ユーザー入力は1-indexed\n",
        "    else:\n",
        "        plot_horizons = [h - 1 for h in pred_horizon_to_plot] # リストの場合も1-indexedを0-indexedに変換\n",
        "\n",
        "    num_plots_per_horizon = 2 # 予測vs実測、累積リターン\n",
        "    total_subplots = len(plot_horizons) * num_plots_per_horizon + 1 # 損失 + 各ホライズンの2つのプロット\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 6 * len(plot_horizons) + 6)) # 全体の図のサイズを調整\n",
        "\n",
        "    # 1. 訓練履歴 (損失)\n",
        "    plt.subplot(total_subplots, 1, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    current_subplot_idx = 2\n",
        "    for p_idx in plot_horizons:\n",
        "        # y_true_seq, y_pred_seq は (N, MAX_PRED_HORIZON) 形状\n",
        "        # p_idx は 0-indexed\n",
        "        y_true_single_horizon = y_true_seq[:, p_idx]\n",
        "        y_pred_single_horizon = y_pred_seq[:, p_idx]\n",
        "\n",
        "        horizon_label = f\"{p_idx + 1}-Day Ahead\" # プロット用ラベル (1-indexed)\n",
        "\n",
        "        # 2. 予測 vs 実測値 (特定期間)\n",
        "        plt.subplot(total_subplots, 1, current_subplot_idx)\n",
        "        display_start_idx = max(0, len(y_true_single_horizon) - 500)\n",
        "        display_dates = dates[display_start_idx:]\n",
        "        display_y_true = y_true_single_horizon[display_start_idx:]\n",
        "        display_y_pred = y_pred_single_horizon[display_start_idx:]\n",
        "\n",
        "        plt.plot(display_dates, display_y_true, label=f'{set_name} Actual Returns', color='blue', alpha=0.7)\n",
        "        plt.plot(display_dates, display_y_pred, label=f'{set_name} Predicted Returns', color='red', linestyle='--', alpha=0.7)\n",
        "        plt.title(f'S&P 500 Daily Returns Prediction ({set_name} Set - {horizon_label} - Last 500 Days)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Daily Return (%)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.gca().xaxis.set_major_locator(mticker.AutoLocator())\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        current_subplot_idx += 1\n",
        "\n",
        "        # 3. 累積リターン (特定期間)\n",
        "        plt.subplot(total_subplots, 1, current_subplot_idx)\n",
        "        cumulative_true = np.cumsum(display_y_true)\n",
        "        cumulative_pred = np.cumsum(display_y_pred)\n",
        "\n",
        "        # 開始点を0に正規化\n",
        "        cumulative_true_normalized = cumulative_true - cumulative_true[0]\n",
        "        cumulative_pred_normalized = cumulative_pred - cumulative_pred[0]\n",
        "\n",
        "        plt.plot(display_dates, cumulative_true_normalized, label=f'{set_name} Actual Cumulative Returns', color='green', linewidth=2)\n",
        "        plt.plot(display_dates, cumulative_pred_normalized, label=f'{set_name} Predicted Cumulative Returns', color='purple', linestyle='--', linewidth=1.5)\n",
        "        plt.title(f'S&P 500 Cumulative Returns ({set_name} Set - {horizon_label} - Last 500 Days)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Cumulative Return (%)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.gca().xaxis.set_major_locator(mticker.AutoLocator())\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        current_subplot_idx += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_direction_accuracy_multi_horizon(y_true_seq, y_pred_seq, set_name):\n",
        "    \"\"\"\n",
        "    方向精度を計算し、各予測ホライズンごとにプロットする関数（多出力対応版）\n",
        "    \"\"\"\n",
        "    num_horizons = y_true_seq.shape[1] # 予測ホライズンの数\n",
        "    accuracies = []\n",
        "\n",
        "    for i in range(num_horizons):\n",
        "        y_true_single = y_true_seq[:, i]\n",
        "        y_pred_single = y_pred_seq[:, i]\n",
        "\n",
        "        true_direction = np.sign(y_true_single)\n",
        "        pred_direction = np.sign(y_pred_single)\n",
        "        correct_directions = np.sum(true_direction == pred_direction)\n",
        "        total_predictions = len(y_true_single)\n",
        "        accuracy = correct_directions / total_predictions * 100\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    horizon_labels = [f'{i+1}D' for i in range(num_horizons)]\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    bars = plt.bar(horizon_labels, accuracies, color='skyblue')\n",
        "    plt.ylabel('Directional Accuracy (%)')\n",
        "    plt.xlabel('Prediction Horizon')\n",
        "    plt.title(f'Directional Accuracy Across Prediction Horizons ({set_name} Set)')\n",
        "    plt.ylim(0, 100) # 0%から100%の範囲\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 1, round(yval, 2), ha='center', va='bottom') # accuracy値を表示\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 訓練・検証・テストセットの結果をプロット\n",
        "# 各ホライズン（1日後、5日後、30日後）の予測を個別にプロット\n",
        "plot_results_multi_horizon(history, y_train_seq, train_preds, dates_train, 'Train', pred_horizon_to_plot=[1, 5, 30])\n",
        "plot_results_multi_horizon(history, y_valid_seq, valid_preds, dates_valid, 'Validation', pred_horizon_to_plot=[1, 5, 30])\n",
        "plot_results_multi_horizon(history, y_test_seq, test_preds, dates_test, 'Test', pred_horizon_to_plot=[1, 5, 30])\n",
        "\n",
        "# 方向精度を全てのホライズンでプロット\n",
        "plot_direction_accuracy_multi_horizon(y_train_seq, train_preds, 'Train')\n",
        "plot_direction_accuracy_multi_horizon(y_valid_seq, valid_preds, 'Validation')\n",
        "plot_direction_accuracy_multi_horizon(y_test_seq, test_preds, 'Test')"
      ],
      "metadata": {
        "id": "DDPrsir-ZlKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}