{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrhqzJB8CnJmLg0wJ9ZGl9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install uv\n",
        "!uv pip install tensorflow\n",
        "!uv pip install fredapi\n",
        "!uv pip install yfinance\n",
        "!uv pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRVx2nyFsGHp",
        "outputId": "a75dd221-157d-4cd2-a1af-2587a3ba54f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uv in /usr/local/lib/python3.11/dist-packages (0.7.19)\n",
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 139ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 87ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 88ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from fredapi import Fred\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "w1G-BKDl5Uxk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 設定とAPIキー ---\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# .env ファイルから環境変数をロード\n",
        "load_dotenv()\n",
        "\n",
        "# --- 設定とAPIキー ---\n",
        "# FRED APIキーを環境変数から取得\n",
        "fred_api_key = os.getenv('FRED_API_KEY')\n",
        "if fred_api_key is None:\n",
        "    raise ValueError(\"FRED_API_KEY が .env ファイルに設定されていません。\")\n",
        "\n",
        "fred = Fred(api_key=fred_api_key) # APIキーを初期化\n",
        "\n",
        "\n",
        "# データの開始日と終了日\n",
        "START_DATE = '2000-01-01'\n",
        "END_DATE = datetime.date.today().strftime('%Y-%m-%d') # 現在の日付まで"
      ],
      "metadata": {
        "id": "l7SAJsL15W9I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Google Driveを安全にマウント ---\n",
        "# COLAB_ENVIRONMENTフラグを利用して、Colab環境でのみGoogle Drive関連の処理を行う\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    COLAB_ENVIRONMENT = True\n",
        "    MOUNT_POINT = '/content/drive'\n",
        "except ImportError:\n",
        "    COLAB_ENVIRONMENT = False\n",
        "    MOUNT_POINT = None # Colab環境でなければマウントポイントは不要\n",
        "\n",
        "def safe_drive_mount(mount_point):\n",
        "    \"\"\"Google Driveをマウントし、既にマウントされている場合はその旨を伝える。\"\"\"\n",
        "    try:\n",
        "        drive.mount(mount_point)\n",
        "        print('[INFO] Google Driveをマウントしました。')\n",
        "        return True # マウント成功\n",
        "    except Exception as e:\n",
        "        msg = str(e)\n",
        "        if 'already mounted' in msg or 'must not already contain files' in msg:\n",
        "            print('[INFO] Google Driveはすでにマウントされています。')\n",
        "            return True # すでにマウント済みと判断\n",
        "        else:\n",
        "            print(f'[ERROR] Google Driveのマウント中にエラーが発生しました: {e}')\n",
        "            return False # マウント失敗"
      ],
      "metadata": {
        "id": "kTnFxWd9Zdpx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "# --- 基本ディレクトリ設定 ---\n",
        "if COLAB_ENVIRONMENT:\n",
        "    # Google Colab環境の場合\n",
        "    if safe_drive_mount(MOUNT_POINT):\n",
        "        BASE_DIR = os.path.join(MOUNT_POINT, 'MyDrive','S&P500_prediction')\n",
        "    else:\n",
        "        # マウントに失敗した場合のフォールバック\n",
        "        print(\"[WARNING] Google Driveのマウントに失敗したため、ローカルディレクトリを使用します。\")\n",
        "        BASE_DIR = './'\n",
        "else:\n",
        "    # Google Colab環境ではない場合（ローカルPCなど）\n",
        "    print(\"[INFO] Google Colab環境ではないため、ローカルディレクトリを使用します。\")\n",
        "    BASE_DIR = './' # カレントディレクトリに作成\n",
        "\n",
        "os.makedirs(BASE_DIR, exist_ok=True) # ディレクトリが存在しない場合は作成\n",
        "print(f\"[INFO] データ保存先ディレクトリ: {BASE_DIR}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAFmkAq6KGzB",
        "outputId": "392fdac9-3aa9-45b9-9fb6-e61ef23e3e2b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INFO] Google Driveをマウントしました。\n",
            "[INFO] データ保存先ディレクトリ: /content/drive/MyDrive/S&P500_prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 補完済みの情報の取得"
      ],
      "metadata": {
        "id": "Kn520oNf538J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CSVファイルからのデータロード ---\n",
        "# ファイル名の設定\n",
        "file_name_csv = 'market_data.csv'\n",
        "\n",
        "# 保存パスの結合\n",
        "file_path_csv = os.path.join(BASE_DIR, file_name_csv)\n",
        "\n",
        "print(f\"\\n--- CSVファイルからデータをロード中: {file_path_csv} ---\")\n",
        "loaded_data_csv = pd.DataFrame()\n",
        "try:\n",
        "    # CSVファイルをロード\n",
        "    # index_col=0 は、CSVの最初の列（日付）をDataFrameのインデックスとして読み込むことを指定します。\n",
        "    # parse_dates=True は、インデックスの列を日付型に変換することを指定します。\n",
        "    loaded_data_csv = pd.read_csv(file_path_csv, index_col=0, parse_dates=True)\n",
        "    print(\"[INFO] CSVファイルからデータが正常にロードされました。\")\n",
        "\n",
        "    # ロードしたデータの最初の数行を表示して確認\n",
        "    print(\"\\nロードされたデータの最初の5行:\")\n",
        "    display(loaded_data_csv)\n",
        "\n",
        "    # ロードしたデータの情報（データ型など）を確認\n",
        "    print(\"\\nロードされたデータの概要:\")\n",
        "    print(loaded_data_csv.info())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"[ERROR] ファイルが見つかりません: {file_path_csv}\")\n",
        "    print(\"指定されたパスにCSVファイルが存在するか確認してください。\")\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] CSVファイルのロード中にエラーが発生しました: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "8fbuWlFsTU9M",
        "outputId": "7853f513-3eaf-4d38-ef62-cb100db7f391"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- CSVファイルからデータをロード中: /content/drive/MyDrive/S&P500_prediction/market_data.csv ---\n",
            "[INFO] CSVファイルからデータが正常にロードされました。\n",
            "\n",
            "ロードされたデータの最初の5行:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            Industrial Production  Unemployment Rate  CPI (All Urban)  \\\n",
              "2000-01-01                91.4092                4.0           169.30   \n",
              "2000-01-02                91.4092                4.0           169.30   \n",
              "2000-01-03                91.4092                4.0           169.30   \n",
              "2000-01-04                91.4092                4.0           169.30   \n",
              "2000-01-05                91.4092                4.0           169.30   \n",
              "...                           ...                ...              ...   \n",
              "2025-06-30               103.5948                4.1           320.58   \n",
              "2025-07-01               103.5948                4.1           320.58   \n",
              "2025-07-02               103.5948                4.1           320.58   \n",
              "2025-07-03               103.5948                4.1           320.58   \n",
              "2025-07-04               103.5948                4.1           320.58   \n",
              "\n",
              "            Federal Funds Rate  10-Year Treasury Yield  2-Year Treasury Yield  \\\n",
              "2000-01-01                5.45                    6.58                   6.38   \n",
              "2000-01-02                5.45                    6.58                   6.38   \n",
              "2000-01-03                5.45                    6.58                   6.38   \n",
              "2000-01-04                5.45                    6.49                   6.30   \n",
              "2000-01-05                5.45                    6.62                   6.38   \n",
              "...                        ...                     ...                    ...   \n",
              "2025-06-30                4.33                    4.24                   3.72   \n",
              "2025-07-01                4.33                    4.26                   3.78   \n",
              "2025-07-02                4.33                    4.30                   3.78   \n",
              "2025-07-03                4.33                    4.30                   3.78   \n",
              "2025-07-04                4.33                    4.30                   3.78   \n",
              "\n",
              "            VIX Index  Oil Price WTI  Consumer Sentiment  M2 Money Stock  ...  \\\n",
              "2000-01-01      24.21          25.56               112.0          4667.6  ...   \n",
              "2000-01-02      24.21          25.56               112.0          4667.6  ...   \n",
              "2000-01-03      24.21          25.56               112.0          4667.6  ...   \n",
              "2000-01-04      27.01          25.56               112.0          4667.6  ...   \n",
              "2000-01-05      26.41          24.65               112.0          4667.6  ...   \n",
              "...               ...            ...                 ...             ...  ...   \n",
              "2025-06-30      16.73          66.30                52.2         21942.0  ...   \n",
              "2025-07-01      16.83          66.30                52.2         21942.0  ...   \n",
              "2025-07-02      16.64          66.30                52.2         21942.0  ...   \n",
              "2025-07-03      16.64          66.30                52.2         21942.0  ...   \n",
              "2025-07-04      16.64          66.30                52.2         21942.0  ...   \n",
              "\n",
              "            Utilities Select Sector SPDR Fund  \\\n",
              "2000-01-01                          11.261680   \n",
              "2000-01-02                          11.261680   \n",
              "2000-01-03                          11.261680   \n",
              "2000-01-04                          10.921969   \n",
              "2000-01-05                          11.197584   \n",
              "...                                       ...   \n",
              "2025-06-30                          81.660000   \n",
              "2025-07-01                          81.940000   \n",
              "2025-07-02                          81.230000   \n",
              "2025-07-03                          81.840000   \n",
              "2025-07-04                          81.840000   \n",
              "\n",
              "            Real Estate Select Sector SPDR Fund  \\\n",
              "2000-01-01                            21.580572   \n",
              "2000-01-02                            21.580572   \n",
              "2000-01-03                            21.580572   \n",
              "2000-01-04                            21.580572   \n",
              "2000-01-05                            21.580572   \n",
              "...                                         ...   \n",
              "2025-06-30                            41.420000   \n",
              "2025-07-01                            41.700000   \n",
              "2025-07-02                            41.780000   \n",
              "2025-07-03                            41.800000   \n",
              "2025-07-04                            41.800000   \n",
              "\n",
              "            Communication Services Select Sector SPDR Fund  \\\n",
              "2000-01-01                                       46.785187   \n",
              "2000-01-02                                       46.785187   \n",
              "2000-01-03                                       46.785187   \n",
              "2000-01-04                                       46.785187   \n",
              "2000-01-05                                       46.785187   \n",
              "...                                                    ...   \n",
              "2025-06-30                                      108.530000   \n",
              "2025-07-01                                      107.760000   \n",
              "2025-07-02                                      107.450000   \n",
              "2025-07-03                                      108.040000   \n",
              "2025-07-04                                      108.040000   \n",
              "\n",
              "            iShares MSCI EAFE ETF  iShares MSCI Emerging Markets ETF  \\\n",
              "2000-01-01              22.604351                           7.340264   \n",
              "2000-01-02              22.604351                           7.340264   \n",
              "2000-01-03              22.604351                           7.340264   \n",
              "2000-01-04              22.604351                           7.340264   \n",
              "2000-01-05              22.604351                           7.340264   \n",
              "...                           ...                                ...   \n",
              "2025-06-30              89.390000                          48.240000   \n",
              "2025-07-01              89.240000                          48.330000   \n",
              "2025-07-02              89.500000                          48.540000   \n",
              "2025-07-03              89.520000                          48.760000   \n",
              "2025-07-04              89.520000                          48.760000   \n",
              "\n",
              "            FXI (China Large-Cap ETF)  EWJ (Japan ETF)  \\\n",
              "2000-01-01                  11.356076        43.690880   \n",
              "2000-01-02                  11.356076        43.690880   \n",
              "2000-01-03                  11.356076        43.690880   \n",
              "2000-01-04                  11.356076        42.510036   \n",
              "2000-01-05                  11.356076        41.329216   \n",
              "...                               ...              ...   \n",
              "2025-06-30                  36.760000        74.970000   \n",
              "2025-07-01                  36.830000        74.420000   \n",
              "2025-07-02                  36.690000        74.460000   \n",
              "2025-07-03                  36.270000        74.600000   \n",
              "2025-07-04                  36.270000        74.600000   \n",
              "\n",
              "            CBOE Interest Rate 10-Year T-Note  \\\n",
              "2000-01-01                              6.548   \n",
              "2000-01-02                              6.548   \n",
              "2000-01-03                              6.548   \n",
              "2000-01-04                              6.485   \n",
              "2000-01-05                              6.599   \n",
              "...                                       ...   \n",
              "2025-06-30                              4.230   \n",
              "2025-07-01                              4.251   \n",
              "2025-07-02                              4.293   \n",
              "2025-07-03                              4.348   \n",
              "2025-07-04                              4.348   \n",
              "\n",
              "            CBOE Interest Rate 2-Year T-Note  US Dollar Index (DXY)  \n",
              "2000-01-01                             5.270                 100.22  \n",
              "2000-01-02                             5.270                 100.22  \n",
              "2000-01-03                             5.270                 100.22  \n",
              "2000-01-04                             5.270                 100.41  \n",
              "2000-01-05                             5.270                 100.38  \n",
              "...                                      ...                    ...  \n",
              "2025-06-30                             4.190                  96.88  \n",
              "2025-07-01                             4.225                  96.82  \n",
              "2025-07-02                             4.223                  96.78  \n",
              "2025-07-03                             4.240                  97.18  \n",
              "2025-07-04                             4.240                  97.18  \n",
              "\n",
              "[9317 rows x 186 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4eb9818-ca9e-4de4-a7dd-05ef8a4b98a6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Industrial Production</th>\n",
              "      <th>Unemployment Rate</th>\n",
              "      <th>CPI (All Urban)</th>\n",
              "      <th>Federal Funds Rate</th>\n",
              "      <th>10-Year Treasury Yield</th>\n",
              "      <th>2-Year Treasury Yield</th>\n",
              "      <th>VIX Index</th>\n",
              "      <th>Oil Price WTI</th>\n",
              "      <th>Consumer Sentiment</th>\n",
              "      <th>M2 Money Stock</th>\n",
              "      <th>...</th>\n",
              "      <th>Utilities Select Sector SPDR Fund</th>\n",
              "      <th>Real Estate Select Sector SPDR Fund</th>\n",
              "      <th>Communication Services Select Sector SPDR Fund</th>\n",
              "      <th>iShares MSCI EAFE ETF</th>\n",
              "      <th>iShares MSCI Emerging Markets ETF</th>\n",
              "      <th>FXI (China Large-Cap ETF)</th>\n",
              "      <th>EWJ (Japan ETF)</th>\n",
              "      <th>CBOE Interest Rate 10-Year T-Note</th>\n",
              "      <th>CBOE Interest Rate 2-Year T-Note</th>\n",
              "      <th>US Dollar Index (DXY)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2000-01-01</th>\n",
              "      <td>91.4092</td>\n",
              "      <td>4.0</td>\n",
              "      <td>169.30</td>\n",
              "      <td>5.45</td>\n",
              "      <td>6.58</td>\n",
              "      <td>6.38</td>\n",
              "      <td>24.21</td>\n",
              "      <td>25.56</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4667.6</td>\n",
              "      <td>...</td>\n",
              "      <td>11.261680</td>\n",
              "      <td>21.580572</td>\n",
              "      <td>46.785187</td>\n",
              "      <td>22.604351</td>\n",
              "      <td>7.340264</td>\n",
              "      <td>11.356076</td>\n",
              "      <td>43.690880</td>\n",
              "      <td>6.548</td>\n",
              "      <td>5.270</td>\n",
              "      <td>100.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-02</th>\n",
              "      <td>91.4092</td>\n",
              "      <td>4.0</td>\n",
              "      <td>169.30</td>\n",
              "      <td>5.45</td>\n",
              "      <td>6.58</td>\n",
              "      <td>6.38</td>\n",
              "      <td>24.21</td>\n",
              "      <td>25.56</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4667.6</td>\n",
              "      <td>...</td>\n",
              "      <td>11.261680</td>\n",
              "      <td>21.580572</td>\n",
              "      <td>46.785187</td>\n",
              "      <td>22.604351</td>\n",
              "      <td>7.340264</td>\n",
              "      <td>11.356076</td>\n",
              "      <td>43.690880</td>\n",
              "      <td>6.548</td>\n",
              "      <td>5.270</td>\n",
              "      <td>100.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-03</th>\n",
              "      <td>91.4092</td>\n",
              "      <td>4.0</td>\n",
              "      <td>169.30</td>\n",
              "      <td>5.45</td>\n",
              "      <td>6.58</td>\n",
              "      <td>6.38</td>\n",
              "      <td>24.21</td>\n",
              "      <td>25.56</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4667.6</td>\n",
              "      <td>...</td>\n",
              "      <td>11.261680</td>\n",
              "      <td>21.580572</td>\n",
              "      <td>46.785187</td>\n",
              "      <td>22.604351</td>\n",
              "      <td>7.340264</td>\n",
              "      <td>11.356076</td>\n",
              "      <td>43.690880</td>\n",
              "      <td>6.548</td>\n",
              "      <td>5.270</td>\n",
              "      <td>100.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-04</th>\n",
              "      <td>91.4092</td>\n",
              "      <td>4.0</td>\n",
              "      <td>169.30</td>\n",
              "      <td>5.45</td>\n",
              "      <td>6.49</td>\n",
              "      <td>6.30</td>\n",
              "      <td>27.01</td>\n",
              "      <td>25.56</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4667.6</td>\n",
              "      <td>...</td>\n",
              "      <td>10.921969</td>\n",
              "      <td>21.580572</td>\n",
              "      <td>46.785187</td>\n",
              "      <td>22.604351</td>\n",
              "      <td>7.340264</td>\n",
              "      <td>11.356076</td>\n",
              "      <td>42.510036</td>\n",
              "      <td>6.485</td>\n",
              "      <td>5.270</td>\n",
              "      <td>100.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-05</th>\n",
              "      <td>91.4092</td>\n",
              "      <td>4.0</td>\n",
              "      <td>169.30</td>\n",
              "      <td>5.45</td>\n",
              "      <td>6.62</td>\n",
              "      <td>6.38</td>\n",
              "      <td>26.41</td>\n",
              "      <td>24.65</td>\n",
              "      <td>112.0</td>\n",
              "      <td>4667.6</td>\n",
              "      <td>...</td>\n",
              "      <td>11.197584</td>\n",
              "      <td>21.580572</td>\n",
              "      <td>46.785187</td>\n",
              "      <td>22.604351</td>\n",
              "      <td>7.340264</td>\n",
              "      <td>11.356076</td>\n",
              "      <td>41.329216</td>\n",
              "      <td>6.599</td>\n",
              "      <td>5.270</td>\n",
              "      <td>100.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-06-30</th>\n",
              "      <td>103.5948</td>\n",
              "      <td>4.1</td>\n",
              "      <td>320.58</td>\n",
              "      <td>4.33</td>\n",
              "      <td>4.24</td>\n",
              "      <td>3.72</td>\n",
              "      <td>16.73</td>\n",
              "      <td>66.30</td>\n",
              "      <td>52.2</td>\n",
              "      <td>21942.0</td>\n",
              "      <td>...</td>\n",
              "      <td>81.660000</td>\n",
              "      <td>41.420000</td>\n",
              "      <td>108.530000</td>\n",
              "      <td>89.390000</td>\n",
              "      <td>48.240000</td>\n",
              "      <td>36.760000</td>\n",
              "      <td>74.970000</td>\n",
              "      <td>4.230</td>\n",
              "      <td>4.190</td>\n",
              "      <td>96.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-01</th>\n",
              "      <td>103.5948</td>\n",
              "      <td>4.1</td>\n",
              "      <td>320.58</td>\n",
              "      <td>4.33</td>\n",
              "      <td>4.26</td>\n",
              "      <td>3.78</td>\n",
              "      <td>16.83</td>\n",
              "      <td>66.30</td>\n",
              "      <td>52.2</td>\n",
              "      <td>21942.0</td>\n",
              "      <td>...</td>\n",
              "      <td>81.940000</td>\n",
              "      <td>41.700000</td>\n",
              "      <td>107.760000</td>\n",
              "      <td>89.240000</td>\n",
              "      <td>48.330000</td>\n",
              "      <td>36.830000</td>\n",
              "      <td>74.420000</td>\n",
              "      <td>4.251</td>\n",
              "      <td>4.225</td>\n",
              "      <td>96.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-02</th>\n",
              "      <td>103.5948</td>\n",
              "      <td>4.1</td>\n",
              "      <td>320.58</td>\n",
              "      <td>4.33</td>\n",
              "      <td>4.30</td>\n",
              "      <td>3.78</td>\n",
              "      <td>16.64</td>\n",
              "      <td>66.30</td>\n",
              "      <td>52.2</td>\n",
              "      <td>21942.0</td>\n",
              "      <td>...</td>\n",
              "      <td>81.230000</td>\n",
              "      <td>41.780000</td>\n",
              "      <td>107.450000</td>\n",
              "      <td>89.500000</td>\n",
              "      <td>48.540000</td>\n",
              "      <td>36.690000</td>\n",
              "      <td>74.460000</td>\n",
              "      <td>4.293</td>\n",
              "      <td>4.223</td>\n",
              "      <td>96.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-03</th>\n",
              "      <td>103.5948</td>\n",
              "      <td>4.1</td>\n",
              "      <td>320.58</td>\n",
              "      <td>4.33</td>\n",
              "      <td>4.30</td>\n",
              "      <td>3.78</td>\n",
              "      <td>16.64</td>\n",
              "      <td>66.30</td>\n",
              "      <td>52.2</td>\n",
              "      <td>21942.0</td>\n",
              "      <td>...</td>\n",
              "      <td>81.840000</td>\n",
              "      <td>41.800000</td>\n",
              "      <td>108.040000</td>\n",
              "      <td>89.520000</td>\n",
              "      <td>48.760000</td>\n",
              "      <td>36.270000</td>\n",
              "      <td>74.600000</td>\n",
              "      <td>4.348</td>\n",
              "      <td>4.240</td>\n",
              "      <td>97.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-04</th>\n",
              "      <td>103.5948</td>\n",
              "      <td>4.1</td>\n",
              "      <td>320.58</td>\n",
              "      <td>4.33</td>\n",
              "      <td>4.30</td>\n",
              "      <td>3.78</td>\n",
              "      <td>16.64</td>\n",
              "      <td>66.30</td>\n",
              "      <td>52.2</td>\n",
              "      <td>21942.0</td>\n",
              "      <td>...</td>\n",
              "      <td>81.840000</td>\n",
              "      <td>41.800000</td>\n",
              "      <td>108.040000</td>\n",
              "      <td>89.520000</td>\n",
              "      <td>48.760000</td>\n",
              "      <td>36.270000</td>\n",
              "      <td>74.600000</td>\n",
              "      <td>4.348</td>\n",
              "      <td>4.240</td>\n",
              "      <td>97.18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9317 rows × 186 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4eb9818-ca9e-4de4-a7dd-05ef8a4b98a6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f4eb9818-ca9e-4de4-a7dd-05ef8a4b98a6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f4eb9818-ca9e-4de4-a7dd-05ef8a4b98a6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1503cad9-3303-48a7-94b0-236fde6041b1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1503cad9-3303-48a7-94b0-236fde6041b1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1503cad9-3303-48a7-94b0-236fde6041b1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f2a5ac50-1217-42c0-a19c-1d3c9874caf2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('loaded_data_csv')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f2a5ac50-1217-42c0-a19c-1d3c9874caf2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('loaded_data_csv');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "loaded_data_csv"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ロードされたデータの概要:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 9317 entries, 2000-01-01 to 2025-07-04\n",
            "Columns: 186 entries, Industrial Production to US Dollar Index (DXY)\n",
            "dtypes: float64(186)\n",
            "memory usage: 13.3 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJKh3Ga9UMix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# グローバル変数"
      ],
      "metadata": {
        "id": "MEWJ7GCvsbPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- グローバル変数設定 ---\n",
        "WINDOW_SIZE = 252 #（約1年間の営業日 # 過去n日間のデータを使用\n",
        "# PRED_HORIZON = 1 # 1日後の予測 (元の設定)\n",
        "MAX_PRED_HORIZON = 30 # 1日から30日後までを予測\n",
        "\n",
        "TEST_SIZE_RATIO = 0.1 # テストデータの割合\n",
        "VALID_SIZE_RATIO = 0.1 # 検証データの割合\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 1 # エポック数を増加させる可能性あり\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "current_target_column = 'SP500_Return'\n",
        "PCA_N_COMPONENTS = 0.95 # PCAで保持する分散の割合\n",
        "\n",
        "print(f\"ターゲットカラム: {current_target_column}\")\n",
        "print(f\"PCA情報保存率の閾値: {PCA_N_COMPONENTS}\")"
      ],
      "metadata": {
        "id": "DAegzijpUXxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dd2323-7d49-40af-c41e-5e3e7b10ce57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ターゲットカラム: SP500_Return\n",
            "PCA情報保存率の閾値: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNL6Dh9sQlMV",
        "outputId": "4efa6bc1-7c40-48e9-9aa7-9bf6e4b2d2d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 163ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import gc"
      ],
      "metadata": {
        "id": "tXgGTkac0HOo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ディレクトリ設定 ---\n",
        "if COLAB_ENVIRONMENT:\n",
        "    # Google Colab環境の場合\n",
        "    if safe_drive_mount(MOUNT_POINT):\n",
        "        BASE_DIR = os.path.join(MOUNT_POINT, 'MyDrive','S&P500_prediction')\n",
        "    else:\n",
        "        # マウントに失敗した場合のフォールバック\n",
        "        print(\"[WARNING] Google Driveのマウントに失敗したため、ローカルディレクトリを使用します。\")\n",
        "        BASE_DIR = './'\n",
        "else:\n",
        "    # Google Colab環境ではない場合（ローカルPCなど）\n",
        "    print(\"[INFO] Google Colab環境ではないため、ローカルディレクトリを使用します。\")\n",
        "    BASE_DIR = './' # カレントディレクトリに作成\n",
        "\n",
        "os.makedirs(BASE_DIR, exist_ok=True) # ディレクトリが存在しない場合は作成\n",
        "print(f\"[INFO] データ保存先ディレクトリ: {BASE_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8o9xv-kWXNG",
        "outputId": "256cf2ae-391a-4470-ef92-ecdf1368bc73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INFO] Google Driveをマウントしました。\n",
            "[INFO] データ保存先ディレクトリ: /content/drive/MyDrive/S&P500_prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 前処理済みデータファイルのパス定義 ---\n",
        "PREPROCESSED_DATA_DIR = os.path.join(BASE_DIR, 'Model_training_in_progress','preprocessed_data')\n",
        "os.makedirs(PREPROCESSED_DATA_DIR, exist_ok=True)\n",
        "\n",
        "X_TRAIN_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'X_train_pca.npy')\n",
        "Y_TRAIN_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'y_train_scaled.npy')\n",
        "DATES_TRAIN_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'dates_train.csv')\n",
        "\n",
        "X_VALID_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'X_valid_pca.npy')\n",
        "Y_VALID_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'y_valid_scaled.npy')\n",
        "DATES_VALID_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'dates_valid.csv')\n",
        "\n",
        "X_TEST_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'X_test_pca.npy')\n",
        "Y_TEST_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'y_test_scaled.npy')\n",
        "DATES_TEST_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'dates_test.csv')\n",
        "\n",
        "# PCAオブジェクトとスケーラーオブジェクトも保存・ロードできるようにパスを定義\n",
        "PCA_MODEL_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'pca_model.pkl')\n",
        "FEATURE_SCALER_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'feature_scaler.pkl')\n",
        "TARGET_SCALER_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'target_scaler.pkl')\n",
        "\n",
        "# scikit-learnオブジェクトの保存・ロードにはjoblibが必要\n",
        "import joblib\n",
        "\n",
        "# --- 前処理済みデータのロードまたは新規作成 ---\n",
        "print(\"\\n--- 前処理済みデータのロードまたは新規作成 ---\")\n",
        "\n",
        "# 全てのファイルが存在するかチェック\n",
        "all_preprocessed_files_exist = (\n",
        "    os.path.exists(X_TRAIN_PATH) and os.path.exists(Y_TRAIN_PATH) and os.path.exists(DATES_TRAIN_PATH) and\n",
        "    os.path.exists(X_VALID_PATH) and os.path.exists(Y_VALID_PATH) and os.path.exists(DATES_VALID_PATH) and\n",
        "    os.path.exists(X_TEST_PATH) and os.path.exists(Y_TEST_PATH) and os.path.exists(DATES_TEST_PATH) and\n",
        "    os.path.exists(PCA_MODEL_PATH) and os.path.exists(FEATURE_SCALER_PATH) and os.path.exists(TARGET_SCALER_PATH)\n",
        ")\n",
        "\n",
        "if all_preprocessed_files_exist:\n",
        "    print(\"[INFO] 既存の前処理済みデータが見つかりました。ロードします。\")\n",
        "    X_train_pca = np.load(X_TRAIN_PATH)\n",
        "    y_train_scaled = np.load(Y_TRAIN_PATH)\n",
        "    dates_train_full = pd.read_csv(DATES_TRAIN_PATH, index_col=0, parse_dates=True).index\n",
        "\n",
        "    X_valid_pca = np.load(X_VALID_PATH)\n",
        "    y_valid_scaled = np.load(Y_VALID_PATH)\n",
        "    dates_valid_full = pd.read_csv(DATES_VALID_PATH, index_col=0, parse_dates=True).index\n",
        "\n",
        "    X_test_pca = np.load(X_TEST_PATH)\n",
        "    y_test_scaled = np.load(Y_TEST_PATH)\n",
        "    dates_test_full = pd.read_csv(DATES_TEST_PATH, index_col=0, parse_dates=True).index\n",
        "\n",
        "    # スケーラーとPCAモデルもロード\n",
        "    pca = joblib.load(PCA_MODEL_PATH)\n",
        "    feature_scaler = joblib.load(FEATURE_SCALER_PATH)\n",
        "    target_scaler = joblib.load(TARGET_SCALER_PATH)\n",
        "\n",
        "    feature_cols = feature_scaler.feature_names_in_ # feature_scalerから元の特徴量名を取得\n",
        "    original_feature_dim = len(feature_cols)\n",
        "\n",
        "    print(f\"ロードされた訓練データ形状: {X_train_pca.shape}, ターゲット形状: {y_train_scaled.shape}\")\n",
        "    print(f\"情報保存率:{PCA_N_COMPONENTS}_PCA後の特徴量次元数: {pca.n_components_} (元の次元: {original_feature_dim})\")\n",
        "\n",
        "else:\n",
        "    print(\"[INFO] 前処理済みデータが見つかりません。新規に作成します。\")\n",
        "    # --- CSVファイルからのデータロード (前のコードからコピー) ---\n",
        "    file_name_csv = 'market_data.csv'\n",
        "    # BASE_DIRの親ディレクトリからロードするようにパスを調整\n",
        "    file_path_csv = os.path.join(BASE_DIR, file_name_csv)\n",
        "\n",
        "    print(f\"\\n--- CSVファイルからデータをロード中: {file_path_csv} ---\")\n",
        "\n",
        "    loaded_data_csv = None\n",
        "    try:\n",
        "        loaded_data_csv = pd.read_csv(file_path_csv, index_col=0, parse_dates=True)\n",
        "        print(\"[INFO] CSVファイルからデータが正常にロードされました。\")\n",
        "        print(\"\\nロードされたデータの最初の5行:\")\n",
        "        display(loaded_data_csv.head())\n",
        "        print(\"\\nロードされたデータの概要:\")\n",
        "        print(loaded_data_csv.info())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[ERROR] ファイルが見つかりません: {file_path_csv}\")\n",
        "        print(\"指定されたパスにCSVファイルが存在するか確認してください。\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] CSVファイルのロード中にエラーが発生しました: {e}\")\n",
        "\n",
        "    if loaded_data_csv is None:\n",
        "        print(\"[ERROR] データロードに失敗したため、処理を終了します。\")\n",
        "        exit()\n",
        "\n",
        "    print(\"\\n--- 1. ロードしたデータに対して最終的な前処理を行います ---\")\n",
        "\n",
        "    if 'S&P500' not in loaded_data_csv.columns:\n",
        "        raise ValueError(\"S&P500 カラムがロードされたデータフレームに見つかりません。\")\n",
        "    loaded_data_csv['S&P500'] = loaded_data_csv['S&P500'].astype(np.float32)\n",
        "    loaded_data_csv[current_target_column] = loaded_data_csv['S&P500'].pct_change(1).shift(-1)\n",
        "\n",
        "    initial_rows_before_target_dropna = loaded_data_csv.shape[0]\n",
        "    loaded_data_csv.dropna(subset=[current_target_column], inplace=True)\n",
        "    rows_deleted_for_target = initial_rows_before_target_dropna - loaded_data_csv.shape[0]\n",
        "    if rows_deleted_for_target > 0:\n",
        "        print(f\"ターゲット ({current_target_column}) のNaNを削除した結果、{rows_deleted_for_target} 行が削除されました。\")\n",
        "\n",
        "    initial_rows_after_target_dropna = loaded_data_csv.shape[0]\n",
        "    loaded_data_csv.dropna(how='all', inplace=True)\n",
        "    rows_deleted_all_nan_after_target = initial_rows_after_target_dropna - loaded_data_csv.shape[0]\n",
        "    if rows_deleted_all_nan_after_target > 0:\n",
        "        print(f\"ターゲットNaN削除後に全てNaNの行を削除した結果、{rows_deleted_all_nan_after_target} 行が削除されました。\")\n",
        "\n",
        "    feature_cols_all = [col for col in loaded_data_csv.columns if col != current_target_column]\n",
        "    full_data_df = loaded_data_csv[feature_cols_all + [current_target_column]].copy()\n",
        "\n",
        "    initial_rows_final = full_data_df.shape[0]\n",
        "    full_data_df.dropna(inplace=True)\n",
        "    rows_deleted_final_features = initial_rows_final - full_data_df.shape[0]\n",
        "    if rows_deleted_final_features > 0:\n",
        "        print(f\"特徴量内の欠損値 ({rows_deleted_final_features} 行) を削除しました。\")\n",
        "\n",
        "    full_data_df = full_data_df.astype(np.float32)\n",
        "\n",
        "    print(f\"最終的なデータ形状 (特徴量とターゲット): {full_data_df.shape}\")\n",
        "    print(f\"最終的なデータセットの欠損値数 (最終確認):\\n{full_data_df.isnull().sum().sum()}\")\n",
        "    if full_data_df.isnull().sum().sum() > 0:\n",
        "        raise ValueError(\"データ前処理後に欠損値が残っています。データ処理を確認してください。\")\n",
        "    print(\"データ前処理完了。\")\n",
        "\n",
        "    del loaded_data_csv\n",
        "    gc.collect()\n",
        "\n",
        "    # --- 3. データを訓練・検証・テストに分割 (時系列順) ---\n",
        "    print(\"\\n--- 3. データ分割 (時系列順) ---\")\n",
        "\n",
        "    feature_cols = [col for col in full_data_df.columns if col != current_target_column]\n",
        "    original_feature_dim = len(feature_cols)\n",
        "    print(f\"PCA適用前の特徴量次元数: {original_feature_dim}\")\n",
        "\n",
        "    train_size = int(len(full_data_df) * (1 - TEST_SIZE_RATIO - VALID_SIZE_RATIO))\n",
        "    valid_size = int(len(full_data_df) * VALID_SIZE_RATIO)\n",
        "    test_size = len(full_data_df) - train_size - valid_size\n",
        "\n",
        "    train_df = full_data_df.iloc[:train_size].copy()\n",
        "    valid_df = full_data_df.iloc[train_size:train_size + valid_size].copy()\n",
        "    test_df = full_data_df.iloc[train_size + valid_size:].copy()\n",
        "\n",
        "    del full_data_df\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"データ分割: 訓練={len(train_df)} 検証={len(valid_df)} テスト={len(test_df)}\")\n",
        "\n",
        "    dates_train_full = train_df.index.copy()\n",
        "    dates_valid_full = valid_df.index.copy()\n",
        "    dates_test_full = test_df.index.copy()\n",
        "\n",
        "    # --- 4. 特徴量スケーリングとPCA適用 ---\n",
        "    print(\"\\n--- 4. 特徴量スケーリングとPCA適用 ---\")\n",
        "\n",
        "    feature_scaler = StandardScaler()\n",
        "    X_train_scaled = feature_scaler.fit_transform(train_df[feature_cols])\n",
        "    X_valid_scaled = feature_scaler.transform(valid_df[feature_cols])\n",
        "    X_test_scaled = feature_scaler.transform(test_df[feature_cols])\n",
        "\n",
        "    y_train_raw = train_df[[current_target_column]].values\n",
        "    y_valid_raw = valid_df[[current_target_column]].values\n",
        "    y_test_raw = test_df[[current_target_column]].values\n",
        "\n",
        "    del train_df\n",
        "    del valid_df\n",
        "    del test_df\n",
        "    gc.collect()\n",
        "\n",
        "    pca = PCA(n_components=PCA_N_COMPONENTS)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled).astype(np.float32)\n",
        "    X_valid_pca = pca.transform(X_valid_scaled).astype(np.float32)\n",
        "    X_test_pca = pca.transform(X_test_scaled).astype(np.float32)\n",
        "\n",
        "    del X_train_scaled\n",
        "    del X_valid_scaled\n",
        "    del X_test_scaled\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"情報保存率:{PCA_N_COMPONENTS}_PCA後の特徴量次元数: {pca.n_components_} (元の次元: {original_feature_dim})\")\n",
        "\n",
        "    target_scaler = StandardScaler()\n",
        "    y_train_scaled = target_scaler.fit_transform(y_train_raw).astype(np.float32)\n",
        "    y_valid_scaled = target_scaler.transform(y_valid_raw).astype(np.float32)\n",
        "    y_test_scaled = target_scaler.transform(y_test_raw).astype(np.float32)\n",
        "\n",
        "    del y_train_raw, y_valid_raw, y_test_raw\n",
        "    gc.collect()\n",
        "\n",
        "    # --- 前処理済みデータの保存 ---\n",
        "    print(\"\\n[INFO] 前処理済みデータをGoogle Driveに保存中...\")\n",
        "    np.save(X_TRAIN_PATH, X_train_pca)\n",
        "    np.save(Y_TRAIN_PATH, y_train_scaled)\n",
        "    pd.DataFrame(index=dates_train_full).to_csv(DATES_TRAIN_PATH)\n",
        "\n",
        "    np.save(X_VALID_PATH, X_valid_pca)\n",
        "    np.save(Y_VALID_PATH, y_valid_scaled)\n",
        "    pd.DataFrame(index=dates_valid_full).to_csv(DATES_VALID_PATH)\n",
        "\n",
        "    np.save(X_TEST_PATH, X_test_pca)\n",
        "    np.save(Y_TEST_PATH, y_test_scaled)\n",
        "    pd.DataFrame(index=dates_test_full).to_csv(DATES_TEST_PATH)\n",
        "\n",
        "    # スケーラーとPCAモデルも保存\n",
        "    joblib.dump(pca, PCA_MODEL_PATH)\n",
        "    joblib.dump(feature_scaler, FEATURE_SCALER_PATH)\n",
        "    joblib.dump(target_scaler, TARGET_SCALER_PATH)\n",
        "    print(\"[INFO] 前処理済みデータの保存が完了しました。\")\n",
        "\n",
        "\n",
        "# --- 4.1. PCAローディングの詳細な出力 ---\n",
        "print(\"\\n--- 各主成分に最も寄与する上位10特徴量 ---\")\n",
        "\n",
        "if pca.components_ is not None and len(feature_cols) == pca.components_.shape[1]:\n",
        "    for i, pc_loadings in enumerate(pca.components_):\n",
        "        pc_index = i + 1\n",
        "        loadings_series = pd.Series(pc_loadings, index=feature_cols)\n",
        "        top_10_features = loadings_series.abs().nlargest(10).index\n",
        "        top_10_loadings_values = loadings_series[top_10_features]\n",
        "\n",
        "        if i < len(pca.explained_variance_ratio_):\n",
        "            contribution_ratio = pca.explained_variance_ratio_[i] * 100\n",
        "        else:\n",
        "            contribution_ratio = 0.0\n",
        "\n",
        "        print(f\"\\n--- 主成分 PC{pc_index} (寄与率: {contribution_ratio:.2f}%) ---\")\n",
        "        for feature_name, loading_value in top_10_loadings_values.items():\n",
        "            print(f\"   - {feature_name}: {loading_value:.4f}\")\n",
        "else:\n",
        "    print(\"PCA components_のデータが利用できないか、特徴量名の長さが一致しません。\")\n",
        "    print(\"PCA components_ shape:\", pca.components_.shape if pca.components_ is not None else \"None\")\n",
        "    print(\"feature_cols length:\", len(feature_cols))\n",
        "\n",
        "# --- 5. シーケンスデータの準備 (`prepare_sequences`) をジェネレータ化 ---\n",
        "print(\"\\n--- 5. シーケンスデータを準備中 (ジェネレータ使用) ---\")\n",
        "\n",
        "def prepare_sequences_generator(features_array, targets_array, dates_series, window_size, max_pred_horizon):\n",
        "    for i in range(len(features_array) - window_size - max_pred_horizon + 1):\n",
        "        target_returns_slice = targets_array[i + window_size : i + window_size + max_pred_horizon]\n",
        "        if len(target_returns_slice) == max_pred_horizon:\n",
        "            yield (features_array[i:(i + window_size)].astype(np.float32),\n",
        "                   target_returns_slice.astype(np.float32))\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(WINDOW_SIZE, X_train_pca.shape[1]), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(MAX_PRED_HORIZON,), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "# TensorFlow Datasetの作成は、データがロードまたは作成された後に実行\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: prepare_sequences_generator(X_train_pca, y_train_scaled.flatten(), dates_train_full, WINDOW_SIZE, MAX_PRED_HORIZON),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: prepare_sequences_generator(X_valid_pca, y_valid_scaled.flatten(), dates_valid_full, WINDOW_SIZE, MAX_PRED_HORIZON),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: prepare_sequences_generator(X_test_pca, y_test_scaled.flatten(), dates_test_full, WINDOW_SIZE, MAX_PRED_HORIZON),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ここで元のNumPy配列はTensorFlow Datasetに変換されたので、メモリ解放\n",
        "#del X_train_pca, y_train_scaled, dates_train_full\n",
        "#del X_valid_pca, y_valid_scaled, dates_valid_full\n",
        "#del X_test_pca, y_test_scaled, dates_test_full\n",
        "gc.collect()\n",
        "\n",
        "print(\"TensorFlow Datasetの作成完了。\")\n",
        "\n",
        "# --- 7. PCA Loadingの可視化 (Plotly HTML出力) ---\n",
        "print(\"\\n--- 7. PCA Components Loadingsの可視化 (Plotly HTML出力) ---\")\n",
        "\n",
        "if pca.components_ is not None and len(feature_cols) == pca.components_.shape[1]:\n",
        "    pca_loadings_df = pd.DataFrame(\n",
        "        pca.components_,\n",
        "        columns=feature_cols,\n",
        "        index=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
        "    ).astype(np.float32)\n",
        "\n",
        "    fig = px.imshow(pca_loadings_df,\n",
        "                    labels=dict(x=\"Original Features\", y=\"Principal Components\", color=\"Loading Weight\"),\n",
        "                    x=feature_cols,\n",
        "                    y=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
        "                    color_continuous_scale='RdBu',\n",
        "                    color_continuous_midpoint=0,\n",
        "                    title='PCA Components Loadings: Contribution of Original Features to Principal Components',\n",
        "                    width=1800,\n",
        "                    height=1200\n",
        "                    )\n",
        "\n",
        "    fig.update_traces(hovertemplate='Original Feature: %{x}<br>Principal Component: %{y}<br>Loading: %{z:.2f}<extra></extra>')\n",
        "    fig.update_xaxes(tickangle=90, tickfont=dict(size=8), automargin=True)\n",
        "    fig.update_yaxes(tickfont=dict(size=10), automargin=True)\n",
        "\n",
        "    html_filename = os.path.join(BASE_DIR, 'pca_loadings_heatmap.html')\n",
        "    fig.write_html(html_filename)\n",
        "\n",
        "    print(f\"PCA Loadingsヒートマップが '{html_filename}' として保存されました。ブラウザで開いて確認してください。\")\n",
        "    print(\"ブラウザでは、ズーム、パン、ホバーによる詳細情報の確認が可能です。\")\n",
        "\n",
        "    del pca_loadings_df\n",
        "    del fig\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"PCA components_のデータが利用できないか、特徴量名の長さが一致しません。\")\n",
        "    print(\"PCA components_ shape:\", pca.components_.shape if pca.components_ is not None else \"None\")\n",
        "    print(\"feature_cols length:\", len(feature_cols))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "536ZcIwIX6jl",
        "outputId": "622bd166-5f58-49d7-e968-4ba2233899f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-4163578104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- 前処理済みデータファイルのパス定義 ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPREPROCESSED_DATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Model_training_in_progress'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'preprocessed_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREPROCESSED_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_TRAIN_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREPROCESSED_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X_train_pca.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback # Callbackをインポート\n",
        "\n",
        "# Mixed Precisionを有効にするポリシーを設定\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"Mixed Precision (mixed_float16) を有効にしました。\")\n",
        "\n",
        "# iTransformerBlock クラスの定義\n",
        "# Transformerモデルに、self-Attention機構を入れることで、iTransfomerに\n",
        "class iTransformerBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    iTransformerモデルの基本ブロックを定義します。\n",
        "    標準的なTransformerブロックのSelf-Attention機構を、\n",
        "    「時間軸を特徴量、特徴量次元をトークン」として転置して適用します。\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout_rate, time_steps, **kwargs):\n",
        "        \"\"\"\n",
        "        iTransformerBlockの初期化\n",
        "        Args:\n",
        "            d_model (int): Attention層の内部次元。特徴量（変数）がこの次元に埋め込まれる。\n",
        "            num_heads (int): Multi-Head Attentionのヘッド数。\n",
        "            dropout_rate (float): ドロップアウト率。\n",
        "            time_steps (int): 入力シーケンスの時系列長 (T)。FFNの出力次元として使用。\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.time_steps = time_steps # T (WINDOW_SIZE)\n",
        "\n",
        "        # Multi-Head Self-Attention層の定義\n",
        "        # key_dim=d_model: 各ヘッドのキー/クエリ/バリューの次元（特徴量トークンの埋め込み次元）\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.attn_dropout = Dropout(dropout_rate) # Attention後のドロップアウト\n",
        "        self.attn_norm = LayerNormalization(epsilon=1e-6) # Attention後のLayerNormalization\n",
        "\n",
        "        # Feed-Forward Network (FFN) の深層化\n",
        "        # iTransformer論文の解釈に基づき、FFNは時系列特徴量（T次元）を変換する。\n",
        "        # 深層化のために中間層を追加。\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(d_model * 4, activation='relu'), # 最初のDense層 (活性化関数ReLU)\n",
        "            Dense(d_model * 2, activation='relu'), # <<== 追加：FFNの中間層（深層化のため）\n",
        "            Dense(time_steps) # 最終的なDense層（出力次元は時系列長Tのまま）\n",
        "        ])\n",
        "        self.ffn_dropout = Dropout(dropout_rate) # FFN後のドロップアウト\n",
        "        self.ffn_norm = LayerNormalization(epsilon=1e-6) # FFN後のLayerNormalization\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        iTransformerBlockのフォワードパス\n",
        "        Args:\n",
        "            inputs (tf.Tensor): 入力テンソル。形状は (batch_size, T, D_emb)。\n",
        "                                D_embは特徴量埋め込み後の次元（通常d_model）。\n",
        "            training (bool): 訓練モードかどうかを示すフラグ。ドロップアウトの挙動に影響。\n",
        "        Returns:\n",
        "            tf.Tensor: 処理後の出力テンソル。形状は入力と同じ (batch_size, T, D_emb)。\n",
        "        \"\"\"\n",
        "        # iTransformerの核心: 変数軸と時間軸を転置\n",
        "        # 入力: (batch_size, 時系列長T, 埋め込み特徴量D_emb)\n",
        "        # 転置後: (batch_size, 埋め込み特徴量D_emb, 時系列長T)\n",
        "        # これにより、Attentionが「D_emb個のトークン（変数）、各トークンはT次元の特徴量を持つ」として機能。\n",
        "        inputs_transposed = tf.transpose(inputs, perm=[0, 2, 1])\n",
        "\n",
        "        # Multi-Head Self-Attentionの適用\n",
        "        # クエリ(Q), キー(K), バリュー(V)は全て転置された入力。\n",
        "        # Attentionは埋め込み特徴量D_emb軸（トークン軸）に対して行われる。\n",
        "        attn_output = self.attention(inputs_transposed, inputs_transposed)\n",
        "        attn_output = self.attn_dropout(attn_output, training=training)\n",
        "        # 残差接続 (Residual connection) と層正規化 (LayerNormalization)\n",
        "        attn_output = self.attn_norm(inputs_transposed + attn_output) # (batch_size, D_emb, T)\n",
        "\n",
        "        # Feed-Forward Network (FFN) の適用\n",
        "        # FFNは、Attention後の各トークン（各変数）のT次元の特徴ベクトルを変換する。\n",
        "        ffn_output = self.ffn(attn_output) # (batch_size, D_emb, T)\n",
        "        ffn_output = self.ffn_dropout(ffn_output, training=training)\n",
        "        # 残差接続と層正規化\n",
        "        output = self.ffn_norm(attn_output + ffn_output) # (batch_size, D_emb, T)\n",
        "\n",
        "        # 最終出力を元の形状 (batch_size, T, D_emb) に転置し直す\n",
        "        return tf.transpose(output, perm=[0, 2, 1])\n",
        "\n",
        "\n",
        "def build_itransformer_model(time_steps, feature_dim, d_model, num_heads, num_blocks, dropout_rate, output_horizon):\n",
        "    \"\"\"\n",
        "    iTransformerモデルを構築する。\n",
        "    Args:\n",
        "        time_steps (int): シーケンス長 (T)\n",
        "        feature_dim (int): 特徴量の次元数 (D)\n",
        "        d_model (int): Attention層の内部次元\n",
        "        num_heads (int): Multi-Head Attentionのヘッド数\n",
        "        num_blocks (int): iTransformerブロックの数\n",
        "        dropout_rate (float): ドロップアウト率\n",
        "        output_horizon (int): モデルの出力次元数 (例: 1日後から30日後までの予測なので 30)\n",
        "    Returns:\n",
        "        tf.keras.Model: 構築されたiTransformerモデル\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=(time_steps, feature_dim), dtype=tf.float32)\n",
        "    x = inputs\n",
        "\n",
        "    # Feature embedding (各変数を d_model に線形変換) の深層化\n",
        "    # 各時間ステップの各特徴量に対して適用されるようにTimeDistributedを使用\n",
        "    x = tf.keras.layers.TimeDistributed(Dense(d_model, activation='relu'))(x) # <<== 活性化関数を追加 + 1層目\n",
        "    x = tf.keras.layers.TimeDistributed(Dense(d_model))(x) # <<== 追加：2層目の埋め込み層\n",
        "\n",
        "    # num_blocks が外から渡されるため、これで制御\n",
        "    for _ in range(num_blocks): # <<== NUM_BLOCKS の値で深さが変わる\n",
        "        x = iTransformerBlock(d_model=d_model, num_heads=num_heads,\n",
        "                              dropout_rate=dropout_rate, time_steps=time_steps)(x)\n",
        "\n",
        "    # Global Average Pooling (時間軸T方向に平均を取る)\n",
        "    # (batch_size, T, d_model) -> (batch_size, d_model)\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # 最終的な予測層: 出力次元を output_horizon に変更\n",
        "    # ここも深層化するためにDense層を追加\n",
        "    x = Dense(d_model // 2, activation='relu')(x) # <<== 追加：予測ヘッドの中間層\n",
        "    outputs = Dense(output_horizon)(x) # 1日後から MAX_PRED_HORIZON 日後までのリターンを予測\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# --- カスタムコールバックの定義 ---\n",
        "class CheckpointAndResumeCallback(Callback):\n",
        "    def __init__(self, base_dir, model_name='itransformer_model', initial_epoch=0):\n",
        "        super().__init__()\n",
        "        self.base_dir = base_dir\n",
        "        self.model_path = os.path.join(base_dir, f'{model_name}.keras') # Keras 3.x形式\n",
        "        self.epoch_file = os.path.join(base_dir, 'last_epoch.txt')\n",
        "        self.initial_epoch = initial_epoch\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # モデルの保存\n",
        "        self.model.save(self.model_path)\n",
        "        print(f\"\\n[INFO] モデルをエポック {epoch+1} で保存しました: {self.model_path}\")\n",
        "\n",
        "        # 現在のエポック数を保存\n",
        "        with open(self.epoch_file, 'w') as f:\n",
        "            f.write(str(epoch + 1)) # 次に開始するエポック数を記録\n",
        "        print(f\"[INFO] 現在のエポック数 {epoch+1} を保存しました: {self.epoch_file}\")\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # 訓練開始時に初期エポックをログに出力\n",
        "        if self.initial_epoch > 0:\n",
        "            print(f\"[INFO] 訓練をエポック {self.initial_epoch} から再開します。\")\n",
        "\n",
        "\n",
        "# --- 7. モデルの構築と訓練 ---\n",
        "print(\"--- モデル構築と訓練 ---\")\n",
        "feature_dim_after_pca = pca.n_components_ # PCA後の特徴量次元\n",
        "\n",
        "# iTransformerモデルのハイパーパラメータ\n",
        "\n",
        "# D_MODEL: モデルの埋め込み次元（embedding dimension）または特徴量次元。\n",
        "#          Transformerの各層における入力と出力の次元数を示す。\n",
        "#          この値が大きいほど、モデルはより多くの情報を表現できるが、計算コストとメモリ使用量が増加。\n",
        "#          通常、Attentionメカニズムの入力・出力の次元にも影響します。\n",
        "#          【D_MODELを増やすことの影響】\n",
        "#          モデルが各ステップで処理できる情報の「幅」が広がり、よりリッチな表現を学習できるようになります。\n",
        "#          これにより、複雑な特徴量を捉える能力が向上する可能性があります。\n",
        "D_MODEL = 512\n",
        "\n",
        "# NUM_HEADS: Multi-Head AttentionにおけるAttentionヘッドの数。\n",
        "#            Multi-Head Attentionは、異なる表現サブスペースからの情報を並行して学習するために、\n",
        "#            Attentionメカニズムを複数回（NUM_HEADSの数だけ）並列に実行します。\n",
        "#            これにより、モデルは異なる種類の関係性やパターンを同時に捉えることができます。\n",
        "#            D_MODELは通常、NUM_HEADSで割り切れる必要があります (D_MODEL % NUM_HEADS == 0)。\n",
        "#            【NUM_HEADSを増やすことの影響】\n",
        "#            モデルが同時に異なる側面に注意を向けられるようになり、データの多様な関係性をより深く探求できます。\n",
        "#            これにより、情報の処理能力と表現学習の「深さ」が向上する可能性があります。\n",
        "NUM_HEADS = 16\n",
        "\n",
        "# NUM_BLOCKS: EncoderまたはDecoderブロック（iTransformerの場合は主にEncoderブロック）の数。\n",
        "#             Transformerモデルは通常、複数のIdenticalなブロックを積み重ねて構成されます。\n",
        "#             この値が大きいほど、モデルはより深いネットワークになり、複雑なパターンを学習する能力が高まりますが、\n",
        "#             勾配消失/爆発の問題や学習時間の増加、過学習のリスクも高まります。\n",
        "#             【NUM_BLOCKSを増やすことの影響】\n",
        "#             モデルの「深さ」が直接的に増します。より多くの層を重ねることで、\n",
        "#             入力データからより抽象的で高レベルな特徴を段階的に抽出し、複雑な階層的関係性を学習する能力が高まります。\n",
        "#             これが、ご指摘の「学習の深さが変わる」という点に最も直接的に関連します。\n",
        "NUM_BLOCKS = 4\n",
        "\n",
        "# DROPOUT_RATE: ドロップアウト層のドロップアウト率。\n",
        "#               過学習を防ぐための正則化手法の一つです。\n",
        "#               訓練中に、各層のニューロンの一部をランダムに「ドロップアウト」（無効化）します。\n",
        "#               この値は、ドロップアウトされるニューロンの割合を示します（例: 0.1は10%のニューロンを無効化）。\n",
        "#               ドロップアウトは訓練時にのみ適用され、推論時には適用されません。\n",
        "#               【DROPOUT_RATEの役割】\n",
        "#               「過学習を防ぐ」ために使用されます。\n",
        "#               モデルが特定の訓練データに過度に依存するのを防ぎ、汎化能力を高める効果があります。\n",
        "#               特にD_MODEL, NUM_HEADS, NUM_BLOCKSを増やすことでモデルの表現力が高まり、\n",
        "#               過学習のリスクも増大するため、ドロップアウトのような正則化手法の重要性が増します。\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "# --- 学習再開ロジック ---\n",
        "model = None\n",
        "initial_epoch = 0\n",
        "checkpoint_model_path = os.path.join(BASE_DIR, 'itransformer_model.keras') # Keras 3.x形式のパス\n",
        "last_epoch_file = os.path.join(BASE_DIR, 'last_epoch.txt')\n",
        "\n",
        "print(\"\\n--- 学習再開チェック ---\")\n",
        "if os.path.exists(checkpoint_model_path) and os.path.exists(last_epoch_file):\n",
        "    try:\n",
        "        # モデルのロード\n",
        "        model = tf.keras.models.load_model(checkpoint_model_path,\n",
        "                                           custom_objects={'iTransformerBlock': iTransformerBlock})\n",
        "        print(f\"[INFO] 既存のモデルをロードしました: {checkpoint_model_path}\")\n",
        "\n",
        "        # 最終エポック数のロード\n",
        "        with open(last_epoch_file, 'r') as f:\n",
        "            initial_epoch = int(f.read())\n",
        "        print(f\"[INFO] 学習をエポック {initial_epoch} から再開します。\")\n",
        "\n",
        "        # 最適化関数と学習率を再設定 (ロードしたモデルはコンパイル済みの状態が復元されるが、学習率を確実に適用するため)\n",
        "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0), loss='mse')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] モデルまたはエポック数のロード中にエラーが発生しました: {e}\")\n",
        "        print(\"[INFO] 新しいモデルとして学習を開始します。\")\n",
        "        model = None # エラー時は新しくモデルを構築\n",
        "        initial_epoch = 0\n",
        "else:\n",
        "    print(\"[INFO] 保存されたモデルが見つかりませんでした。新しいモデルとして学習を開始します。\")\n",
        "\n",
        "# モデルがロードされなかった場合、新しくモデルを構築\n",
        "if model is None:\n",
        "    model = build_itransformer_model(\n",
        "        time_steps=WINDOW_SIZE,\n",
        "        feature_dim=feature_dim_after_pca,\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=NUM_HEADS,\n",
        "        num_blocks=NUM_BLOCKS,\n",
        "        dropout_rate=DROPOUT_RATE,\n",
        "        output_horizon=MAX_PRED_HORIZON\n",
        "    )\n",
        "    # 新しいモデルの場合はここでコンパイル\n",
        "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0), loss='mse')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# コールバックの設定\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=12, min_lr=5e-8, verbose=1)\n",
        "\n",
        "# カスタムコールバックのインスタンス化\n",
        "checkpoint_callback = CheckpointAndResumeCallback(\n",
        "    base_dir=BASE_DIR,\n",
        "    initial_epoch=initial_epoch # initial_epochを渡すことでログに出力される\n",
        ")\n",
        "\n",
        "\n",
        "# 訓練データセットの総サンプル数（シーケンス数）を計算\n",
        "# len(features_array) - window_size - max_pred_horizon + 1\n",
        "num_train_sequences = len(X_train_pca) - WINDOW_SIZE - MAX_PRED_HORIZON + 1\n",
        "num_valid_sequences = len(X_valid_pca) - WINDOW_SIZE - MAX_PRED_HORIZON + 1\n",
        "\n",
        "\n",
        "# steps_per_epoch を計算（訓練データ）\n",
        "train_steps_per_epoch = int(np.ceil(num_train_sequences / BATCH_SIZE))\n",
        "\n",
        "# validation_steps も計算（検証データ）\n",
        "# 検証データは通常エポックごとに全て処理されるため、train_steps_per_epoch と同様に計算します。\n",
        "validation_steps_per_epoch = int(np.ceil(num_valid_sequences / BATCH_SIZE))\n",
        "\n",
        "\n",
        "print(\"モデル訓練を開始します...\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping, reduce_lr, checkpoint_callback], # カスタムコールバックを追加\n",
        "    verbose=1,\n",
        "    initial_epoch=initial_epoch # ここで学習再開エポックを指定\n",
        "    # ここに steps_per_epoch と validation_steps を追加\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    validation_steps=validation_steps_per_epoch\n",
        ")\n",
        "print(\"モデル訓練完了。\")\n",
        "\n",
        "# --- ここから新しい評価と可視化のコードを追加 ---\n",
        "\n",
        "# 評価に必要なライブラリをインポート\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "print(\"\\n--- モデルの評価と予測 ---\")\n",
        "\n",
        "y_pred_scaled = model.predict(test_dataset)\n",
        "\n",
        "y_true_scaled_list = []\n",
        "for _, labels in test_dataset.unbatch().as_numpy_iterator():\n",
        "    y_true_scaled_list.append(labels)\n",
        "y_true_scaled = np.array(y_true_scaled_list)\n",
        "\n",
        "# y_pred_scaled の形状を調整 (MAX_PRED_HORIZONが1の場合、(N, 1)から(N,)にflattenする)\n",
        "# predict()の出力は(N, MAX_PRED_HORIZON)ですが、inverse_transformは(N, 1)のような2D配列を期待します。\n",
        "# y_true_scaled も(N, MAX_PRED_HORIZON)の形状なので、直接inverse_transformに渡します。\n",
        "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
        "y_true = target_scaler.inverse_transform(y_true_scaled)\n",
        "\n",
        "print(f\"元のスケールに戻した予測値の最初の5つ: {y_pred[:5].flatten()}\")\n",
        "print(f\"元のスケールに戻した実際の値の最初の5つ: {y_true[:5].flatten()}\")\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(f\"\\n評価指標:\")\n",
        "print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "\n",
        "print(\"\\n--- 訓練履歴の可視化 ---\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- 予測結果の可視化 ---\")\n",
        "\n",
        "# plot_dates の調整: y_true の長さと一致するようにスライスする\n",
        "# test_dataset が prepare_sequences_generator から生成されているため、\n",
        "# データの長さは len(original_data) - WINDOW_SIZE - MAX_PRED_HORIZON + 1 となります。\n",
        "# dates_test_full からこの開始点以降の日付を取得します。\n",
        "plot_dates_start_idx = WINDOW_SIZE # シーケンスの始まり\n",
        "# plot_dates は y_true の各要素に対応する日付であるべきです。\n",
        "# y_true はテストデータ全体のシーケンスの数になります。\n",
        "plot_dates = dates_test_full[plot_dates_start_idx : plot_dates_start_idx + len(y_true)]\n",
        "\n",
        "if len(plot_dates) != len(y_true):\n",
        "    print(f\"[WARNING] プロットする日付の数 ({len(plot_dates)}) と予測/実際の値の数 ({len(y_true)}) が一致しません。\")\n",
        "    print(\"日付の計算ロジックを確認してください。y_trueの長さに合わせて日付を調整します。\")\n",
        "    # ここでの再調整は、すでに上記の計算で試みられていますが、念のため。\n",
        "    # 最も確実なのは、y_true_scaled_listを構築する際に、対応する日付も一緒に収集することです。\n",
        "    # しかし、ここではシンプルにlen(y_true)に合わせてスライスします。\n",
        "    plot_dates = dates_test_full[WINDOW_SIZE : WINDOW_SIZE + len(y_true)]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(plot_dates, y_true.flatten(), label='Actual Values', color='blue', alpha=0.7)\n",
        "plt.plot(plot_dates, y_pred.flatten(), label='Predicted Values', color='red', alpha=0.7, linestyle='--')\n",
        "plt.title('Actual vs. Predicted S&P500 Returns (Test Set)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('S&P500 Returns')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- 予測誤差の分布 ---\")\n",
        "errors = (y_true - y_pred).flatten()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(errors, kde=True, bins=50)\n",
        "plt.title('Distribution of Prediction Errors')\n",
        "plt.xlabel('Prediction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- 評価と可視化の完了 ---\")\n",
        "\n",
        "# 学習終了後、メモリ解放\n",
        "# PCA, feature_scaler, target_scaler は既にjoblibで保存されているので、delしても問題ない\n",
        "del X_train_pca, y_train_scaled, dates_train_full\n",
        "del X_valid_pca, y_valid_scaled, dates_valid_full\n",
        "del X_test_pca, y_test_scaled, dates_test_full\n",
        "del pca\n",
        "del feature_scaler\n",
        "del target_scaler\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "qchGJJgDsrZa",
        "outputId": "98e02363-01e9-4468-9898-68dabb768ce2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-1-3744406409.py, line 272)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-3744406409.py\"\u001b[0;36m, line \u001b[0;32m272\u001b[0m\n\u001b[0;31m    initial_epoch=initial_epoch # ここで学習再開エポックを指定\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "print(\"--- モデル評価 ---\")\n",
        "train_loss = model.evaluate(train_dataset, verbose=0)\n",
        "valid_loss = model.evaluate(valid_dataset, verbose=0)\n",
        "test_loss = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "print(f\"訓練セットのMSE: {train_loss:.4f}\")\n",
        "print(f\"検証セットのMSE: {valid_loss:.4f}\")\n",
        "print(f\"テストセットのMSE: {test_loss:.4f}\")\n",
        "\n",
        "# 予測の実行 (predsの形状が (N, MAX_PRED_HORIZON) になるため、.flatten() は削除)\n",
        "train_preds = model.predict(train_dataset)\n",
        "valid_preds = model.predict(valid_dataset)\n",
        "test_preds = model.predict(test_dataset)"
      ],
      "metadata": {
        "id": "ZCUVMzHzswFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MSmoPdwby6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps_per_epoch = math.ceil(len(X_train_seq) / BATCH_SIZE) # X_train_seq が prepare_sequences で得られた配列と仮定\n",
        "valid_steps_per_epoch = math.ceil(len(X_valid_seq) / BATCH_SIZE)\n",
        "test_steps_per_epoch = math.ceil(len(X_test_seq) / BATCH_SIZE)\n",
        "\n",
        "\n",
        "# 予測値を元のスケールに戻すヘルパー関数\n",
        "def inverse_transform_predictions(scaled_preds, scaler):\n",
        "    original_shape = scaled_preds.shape\n",
        "    # (N, M) -> (N*M, 1) にreshape (StandardScalerが1次元入力を期待するため)\n",
        "    flat_scaled_preds = scaled_preds.reshape(-1, 1)\n",
        "    # 逆変換\n",
        "    flat_original_preds = scaler.inverse_transform(flat_scaled_preds)\n",
        "    # (N*M, 1) -> (N, M) にreshapeし直す\n",
        "    original_preds = flat_original_preds.reshape(original_shape)\n",
        "    return original_preds\n",
        "\n",
        "# モデルの評価 (損失はスケーリングされたままのMSE)\n",
        "print(\"--- モデル評価 (スケーリングされた損失) ---\")\n",
        "train_loss = model.evaluate(train_dataset, steps=train_steps_per_epoch, verbose=0)\n",
        "valid_loss = model.evaluate(valid_dataset, steps=valid_steps_per_epoch, verbose=0)\n",
        "test_loss = model.evaluate(test_dataset, steps=test_steps_per_epoch, verbose=0)\n",
        "\n",
        "print(f\"訓練セットのMSE (スケーリング済み): {train_loss:.4f}\")\n",
        "print(f\"検証セットのMSE (スケーリング済み): {valid_loss:.4f}\")\n",
        "print(f\"テストセットのMSE (スケーリング済み): {test_loss:.4f}\")\n",
        "\n",
        "\n",
        "# 予測の実行\n",
        "print(\"--- 予測実行 ---\")\n",
        "train_preds_scaled = model.predict(train_dataset, steps=train_steps_per_epoch)\n",
        "valid_preds_scaled = model.predict(valid_dataset, steps=valid_steps_per_epoch)\n",
        "test_preds_scaled = model.predict(test_dataset, steps=test_steps_per_epoch)\n",
        "\n",
        "print(f\"訓練予測形状 (スケーリング済み): {train_preds_scaled.shape}\")\n",
        "print(f\"検証予測形状 (スケーリング済み): {valid_preds_scaled.shape}\")\n",
        "print(f\"テスト予測形状 (スケーリング済み): {test_preds_scaled.shape}\")\n",
        "\n",
        "\n",
        "# 予測値を元のスケールに戻す\n",
        "print(\"--- 予測値の標準化解除 ---\")\n",
        "train_preds_original_scale = inverse_transform_predictions(train_preds_scaled, target_scaler)\n",
        "valid_preds_original_scale = inverse_transform_predictions(valid_preds_scaled, target_scaler)\n",
        "test_preds_original_scale = inverse_transform_predictions(test_preds_scaled, target_scaler)\n",
        "\n",
        "print(f\"訓練予測形状 (元のスケール): {train_preds_original_scale.shape}\")\n",
        "print(f\"検証予測形状 (元のスケール): {valid_preds_original_scale.shape}\")\n",
        "print(f\"テスト予測形状 (元のスケール): {test_preds_original_scale.shape}\")\n",
        "\n",
        "# 実際のターゲット値も元のスケールに戻す（評価やプロットのため）\n",
        "print(\"--- 実際のターゲット値の標準化解除 ---\")\n",
        "# y_train_seq, y_valid_seq, y_test_seq は prepare_sequences で得られたスケーリング済みのターゲット配列\n",
        "actual_train_returns_original_scale = inverse_transform_predictions(y_train_seq, target_scaler)\n",
        "actual_valid_returns_original_scale = inverse_transform_predictions(y_valid_seq, target_scaler)\n",
        "actual_test_returns_original_scale = inverse_transform_predictions(y_test_seq, target_scaler)\n",
        "\n",
        "print(f\"訓練実際のターゲット形状 (元のスケール): {actual_train_returns_original_scale.shape}\")\n",
        "print(f\"検証実際のターゲット形状 (元のスケール): {actual_valid_returns_original_scale.shape}\")\n",
        "print(f\"テスト実際のターゲット形状 (元のスケール): {actual_test_returns_original_scale.shape}\")\n",
        "\n",
        "\n",
        "# 元のスケールでの評価指標の計算 (例: RMSE, MAE)\n",
        "print(\"--- 元のスケールでの評価指標 ---\")\n",
        "\n",
        "# 通常、評価は一番最初の予測ホライズン (H+1) で行われることが多いです\n",
        "# もし複数のホライズンすべてで評価する場合は、ループで回す必要があります\n",
        "horizon_to_evaluate = 0 # 最初の予測ホライズン (H+1)\n",
        "\n",
        "if MAX_PRED_HORIZON > 0:\n",
        "    # 訓練セット\n",
        "    train_rmse = np.sqrt(mean_squared_error(actual_train_returns_original_scale[:, horizon_to_evaluate],\n",
        "                                            train_preds_original_scale[:, horizon_to_evaluate]))\n",
        "    train_mae = mean_absolute_error(actual_train_returns_original_scale[:, horizon_to_evaluate],\n",
        "                                    train_preds_original_scale[:, horizon_to_evaluate])\n",
        "\n",
        "    # 検証セット\n",
        "    valid_rmse = np.sqrt(mean_squared_error(actual_valid_returns_original_scale[:, horizon_to_evaluate],\n",
        "                                            valid_preds_original_scale[:, horizon_to_evaluate]))\n",
        "    valid_mae = mean_absolute_error(actual_valid_returns_original_scale[:, horizon_to_evaluate],\n",
        "                                    valid_preds_original_scale[:, horizon_to_evaluate])\n",
        "\n",
        "    # テストセット\n",
        "    test_rmse = np.sqrt(mean_squared_error(actual_test_returns_original_scale[:, horizon_to_evaluate],\n",
        "                                           test_preds_original_scale[:, horizon_to_evaluate]))\n",
        "    test_mae = mean_absolute_error(actual_test_returns_original_scale[:, horizon_to_evaluate],\n",
        "                                  test_preds_original_scale[:, horizon_to_evaluate])\n",
        "\n",
        "    print(f\"訓練セットのRMSE (H+{horizon_to_evaluate+1}): {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n",
        "    print(f\"検証セットのRMSE (H+{horizon_to_evaluate+1}): {valid_rmse:.4f}, MAE: {valid_mae:.4f}\")\n",
        "    print(f\"テストセットのRMSE (H+{horizon_to_evaluate+1}): {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n",
        "else:\n",
        "    print(\"MAX_PRED_HORIZON が0のため、RMSE/MAEは計算されませんでした。\")\n",
        "\n",
        "\n",
        "# これで、train_preds_original_scale, valid_preds_original_scale, test_preds_original_scale\n",
        "# および actual_train_returns_original_scale, actual_valid_returns_original_scale, actual_test_returns_original_scale\n",
        "# を使って、プロットや詳細な分析を行うことができます。"
      ],
      "metadata": {
        "id": "GGVIqPuOlbQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果の可視化\n",
        "def plot_results_multi_horizon(history, y_true_seq, y_pred_seq, dates, set_name, pred_horizon_to_plot=None):\n",
        "    \"\"\"\n",
        "    訓練履歴と予測結果をプロットする関数（多出力対応版）\n",
        "    Args:\n",
        "        pred_horizon_to_plot (int or list of int, optional): プロットしたい予測ホライズン。\n",
        "                                                             Noneの場合、1日後の予測のみプロット。\n",
        "    \"\"\"\n",
        "    if pred_horizon_to_plot is None:\n",
        "        plot_horizons = [0] # 0-indexed で 1日後の予測\n",
        "    elif isinstance(pred_horizon_to_plot, int):\n",
        "        plot_horizons = [pred_horizon_to_plot - 1] # ユーザー入力は1-indexed\n",
        "    else:\n",
        "        plot_horizons = [h - 1 for h in pred_horizon_to_plot] # リストの場合も1-indexedを0-indexedに変換\n",
        "\n",
        "    num_plots_per_horizon = 2 # 予測vs実測、累積リターン\n",
        "    total_subplots = len(plot_horizons) * num_plots_per_horizon + 1 # 損失 + 各ホライズンの2つのプロット\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 6 * len(plot_horizons) + 6)) # 全体の図のサイズを調整\n",
        "\n",
        "    # 1. 訓練履歴 (損失)\n",
        "    plt.subplot(total_subplots, 1, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    current_subplot_idx = 2\n",
        "    for p_idx in plot_horizons:\n",
        "        # y_true_seq, y_pred_seq は (N, MAX_PRED_HORIZON) 形状\n",
        "        # p_idx は 0-indexed\n",
        "        y_true_single_horizon = y_true_seq[:, p_idx]\n",
        "        y_pred_single_horizon = y_pred_seq[:, p_idx]\n",
        "\n",
        "        horizon_label = f\"{p_idx + 1}-Day Ahead\" # プロット用ラベル (1-indexed)\n",
        "\n",
        "        # 2. 予測 vs 実測値 (特定期間)\n",
        "        plt.subplot(total_subplots, 1, current_subplot_idx)\n",
        "        display_start_idx = max(0, len(y_true_single_horizon) - 500)\n",
        "        display_dates = dates[display_start_idx:]\n",
        "        display_y_true = y_true_single_horizon[display_start_idx:]\n",
        "        display_y_pred = y_pred_single_horizon[display_start_idx:]\n",
        "\n",
        "        plt.plot(display_dates, display_y_true, label=f'{set_name} Actual Returns', color='blue', alpha=0.7)\n",
        "        plt.plot(display_dates, display_y_pred, label=f'{set_name} Predicted Returns', color='red', linestyle='--', alpha=0.7)\n",
        "        plt.title(f'S&P 500 Daily Returns Prediction ({set_name} Set - {horizon_label} - Last 500 Days)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Daily Return (%)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.gca().xaxis.set_major_locator(mticker.AutoLocator())\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        current_subplot_idx += 1\n",
        "\n",
        "        # 3. 累積リターン (特定期間)\n",
        "        plt.subplot(total_subplots, 1, current_subplot_idx)\n",
        "        cumulative_true = np.cumsum(display_y_true)\n",
        "        cumulative_pred = np.cumsum(display_y_pred)\n",
        "\n",
        "        # 開始点を0に正規化\n",
        "        cumulative_true_normalized = cumulative_true - cumulative_true[0]\n",
        "        cumulative_pred_normalized = cumulative_pred - cumulative_pred[0]\n",
        "\n",
        "        plt.plot(display_dates, cumulative_true_normalized, label=f'{set_name} Actual Cumulative Returns', color='green', linewidth=2)\n",
        "        plt.plot(display_dates, cumulative_pred_normalized, label=f'{set_name} Predicted Cumulative Returns', color='purple', linestyle='--', linewidth=1.5)\n",
        "        plt.title(f'S&P 500 Cumulative Returns ({set_name} Set - {horizon_label} - Last 500 Days)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Cumulative Return (%)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.gca().xaxis.set_major_locator(mticker.AutoLocator())\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        current_subplot_idx += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_direction_accuracy_multi_horizon(y_true_seq, y_pred_seq, set_name):\n",
        "    \"\"\"\n",
        "    方向精度を計算し、各予測ホライズンごとにプロットする関数（多出力対応版）\n",
        "    \"\"\"\n",
        "    num_horizons = y_true_seq.shape[1] # 予測ホライズンの数\n",
        "    accuracies = []\n",
        "\n",
        "    for i in range(num_horizons):\n",
        "        y_true_single = y_true_seq[:, i]\n",
        "        y_pred_single = y_pred_seq[:, i]\n",
        "\n",
        "        true_direction = np.sign(y_true_single)\n",
        "        pred_direction = np.sign(y_pred_single)\n",
        "        correct_directions = np.sum(true_direction == pred_direction)\n",
        "        total_predictions = len(y_true_single)\n",
        "        accuracy = correct_directions / total_predictions * 100\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    horizon_labels = [f'{i+1}D' for i in range(num_horizons)]\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    bars = plt.bar(horizon_labels, accuracies, color='skyblue')\n",
        "    plt.ylabel('Directional Accuracy (%)')\n",
        "    plt.xlabel('Prediction Horizon')\n",
        "    plt.title(f'Directional Accuracy Across Prediction Horizons ({set_name} Set)')\n",
        "    plt.ylim(0, 100) # 0%から100%の範囲\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 1, round(yval, 2), ha='center', va='bottom') # accuracy値を表示\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 訓練・検証・テストセットの結果をプロット\n",
        "# 各ホライズン（1日後、5日後、30日後）の予測を個別にプロット\n",
        "plot_results_multi_horizon(history, y_train_seq, train_preds, dates_train, 'Train', pred_horizon_to_plot=[1, 5, 30])\n",
        "plot_results_multi_horizon(history, y_valid_seq, valid_preds, dates_valid, 'Validation', pred_horizon_to_plot=[1, 5, 30])\n",
        "plot_results_multi_horizon(history, y_test_seq, test_preds, dates_test, 'Test', pred_horizon_to_plot=[1, 5, 30])\n",
        "\n",
        "# 方向精度を全てのホライズンでプロット\n",
        "plot_direction_accuracy_multi_horizon(y_train_seq, train_preds, 'Train')\n",
        "plot_direction_accuracy_multi_horizon(y_valid_seq, valid_preds, 'Validation')\n",
        "plot_direction_accuracy_multi_horizon(y_test_seq, test_preds, 'Test')"
      ],
      "metadata": {
        "id": "KOxRTCW8s9Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_SCALING_FACTOR_GLOBAL = 100#10000\n",
        "\n",
        "\n",
        "# 結果の可視化\n",
        "def plot_results_multi_horizon(history, y_true_seq, y_pred_seq, dates, set_name, pred_horizon_to_plot=None, target_scaling_factor=100):\n",
        "    \"\"\"\n",
        "    訓練履歴と予測結果をプロットする関数（多出力対応版）\n",
        "    Args:\n",
        "        history: モデルの訓練履歴オブジェクト (keras.callbacks.History)。\n",
        "        y_true_seq (np.array): 実測値のシーケンスデータ (N, num_horizons)。\n",
        "        y_pred_seq (np.array): 予測値のシーケンスデータ (N, num_horizons)。\n",
        "        dates (pd.DatetimeIndex or list of datetime): 対応する日付データ。\n",
        "        set_name (str): データセットの名前 ('Train', 'Validation', 'Test' など)。\n",
        "        pred_horizon_to_plot (int or list of int, optional): プロットしたい予測ホライズン (1-indexed)。\n",
        "                                                          Noneの場合、0-indexedで1日後の予測 (最初のホライズン) のみプロット。\n",
        "        target_scaling_factor (int/float): preprocess_dataでターゲットをスケーリングした係数。\n",
        "                                           これを元のスケールに戻すために使用。デフォルトは100 (元々%)。\n",
        "    \"\"\"\n",
        "    if pred_horizon_to_plot is None:\n",
        "        plot_horizons = [0] # 0-indexed で 1日後の予測（最初のホライズン）\n",
        "    elif isinstance(pred_horizon_to_plot, int):\n",
        "        plot_horizons = [pred_horizon_to_plot - 1] # ユーザー入力は1-indexedを0-indexedに変換\n",
        "    else:\n",
        "        # リストの場合も1-indexedを0-indexedに変換\n",
        "        plot_horizons = [h - 1 for h in pred_horizon_to_plot if h - 1 < y_true_seq.shape[1]]\n",
        "        if not plot_horizons:\n",
        "            print(f\"警告: 指定された pred_horizon_to_plot {pred_horizon_to_plot} は有効なホライズンではありません。\")\n",
        "            print(f\"y_true_seq のホライズン数は {y_true_seq.shape[1]} です。\")\n",
        "            return\n",
        "\n",
        "    num_plots_per_horizon = 2 # 予測vs実測、累積リターン\n",
        "    total_subplots = len(plot_horizons) * num_plots_per_horizon + 1 # 損失 + 各ホライズンの2つのプロット\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 6 * len(plot_horizons) + 6)) # 全体の図のサイズを調整\n",
        "\n",
        "    # 1. 訓練履歴 (損失)\n",
        "    plt.subplot(total_subplots, 1, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    if 'val_loss' in history.history: # 検証損失がない場合もあるためチェック\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    current_subplot_idx = 2\n",
        "    for p_idx in plot_horizons:\n",
        "        # y_true_seq, y_pred_seq は (N, num_horizons) 形状\n",
        "        # p_idx は 0-indexed\n",
        "\n",
        "        # ★ ここでスケーリングを元に戻す ★\n",
        "        y_true_single_horizon = y_true_seq[:, p_idx] / target_scaling_factor * 100 # 元の%表示に戻す\n",
        "        y_pred_single_horizon = y_pred_seq[:, p_idx] / target_scaling_factor * 100 # 元の%表示に戻す\n",
        "\n",
        "        # ホライズンのラベルは元の日数で表示 (例: 1-Day, 5-Day, 20-Day)\n",
        "        # y_true_seqの2次元目のインデックスp_idxは0から始まるため、p_idx+1が実際の日数に対応\n",
        "        horizon_label_days = p_idx + 1\n",
        "        horizon_title = f\"{horizon_label_days}-Day Ahead\" # プロット用ラベル (1-indexed)\n",
        "\n",
        "        # 2. 予測 vs 実測値 (特定期間)\n",
        "        plt.subplot(total_subplots, 1, current_subplot_idx)\n",
        "        # 最近の500日間のデータを表示\n",
        "        display_start_idx = max(0, len(y_true_single_horizon) - 500)\n",
        "        display_dates = dates[display_start_idx:]\n",
        "        display_y_true = y_true_single_horizon[display_start_idx:]\n",
        "        display_y_pred = y_pred_single_horizon[display_start_idx:]\n",
        "\n",
        "        plt.plot(display_dates, display_y_true, label=f'{set_name} Actual Returns', color='blue', alpha=0.7)\n",
        "        plt.plot(display_dates, display_y_pred, label=f'{set_name} Predicted Returns', color='red', linestyle='--', alpha=0.7)\n",
        "        plt.title(f'S&P 500 Returns Prediction ({set_name} Set - {horizon_title} - Last 500 Days)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Return (%)') # Y軸ラベルもReturn (%) に変更\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.gca().xaxis.set_major_locator(mticker.AutoLocator())\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        current_subplot_idx += 1\n",
        "\n",
        "        # 3. 累積リターン (特定期間)\n",
        "        plt.subplot(total_subplots, 1, current_subplot_idx)\n",
        "        # 累積リターンも元のパーセンテージスケールで計算\n",
        "        cumulative_true = np.cumsum(display_y_true)\n",
        "        cumulative_pred = np.cumsum(display_y_pred)\n",
        "\n",
        "        # 開始点を0に正規化\n",
        "        cumulative_true_normalized = cumulative_true - cumulative_true[0]\n",
        "        cumulative_pred_normalized = cumulative_pred - cumulative_pred[0]\n",
        "\n",
        "        plt.plot(display_dates, cumulative_true_normalized, label=f'{set_name} Actual Cumulative Returns', color='green', linewidth=2)\n",
        "        plt.plot(display_dates, cumulative_pred_normalized, label=f'{set_name} Predicted Cumulative Returns', color='purple', linestyle='--', linewidth=1.5)\n",
        "        plt.title(f'S&P 500 Cumulative Returns ({set_name} Set - {horizon_title} - Last 500 Days)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Cumulative Return (%)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.gca().xaxis.set_major_locator(mticker.AutoLocator())\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        current_subplot_idx += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_direction_accuracy_multi_horizon(y_true_seq, y_pred_seq, set_name, target_scaling_factor=100):\n",
        "    \"\"\"\n",
        "    方向精度を計算し、各予測ホライズンごとにプロットする関数（多出力対応版）\n",
        "    Args:\n",
        "        y_true_seq (np.array): 実測値のシーケンスデータ (N, num_horizons)。\n",
        "        y_pred_seq (np.array): 予測値のシーケンスデータ (N, num_horizons)。\n",
        "        set_name (str): データセットの名前 ('Train', 'Validation', 'Test' など)。\n",
        "        target_scaling_factor (int/float): preprocess_dataでターゲットをスケーリングした係数。\n",
        "                                           これを元のスケールに戻すために使用。デフォルトは100 (元々%)。\n",
        "    \"\"\"\n",
        "    num_horizons = y_true_seq.shape[1] # 予測ホライズンの数\n",
        "    accuracies = []\n",
        "\n",
        "    for i in range(num_horizons):\n",
        "        # ★ ここでスケーリングを元に戻す ★\n",
        "        # 方向精度を計算する際には、絶対値のスケールは関係ないため、\n",
        "        # スケーリングを元に戻す必要は厳密にはありません。\n",
        "        # np.sign() は正、負、ゼロを判断するため、スケーリングの影響を受けません。\n",
        "        # しかし、一貫性のためにここでも元のパーセンテージリターンに換算するコードは残しておきます。\n",
        "        # 必要に応じて、y_true_seq[:, i] / target_scaling_factor * 100 を y_true_seq[:, i] に戻しても問題ありません。\n",
        "        y_true_single = y_true_seq[:, i] # / target_scaling_factor * 100 # Directional accuracy doesn't need scaling\n",
        "        y_pred_single = y_pred_seq[:, i] # / target_scaling_factor * 100 # Directional accuracy doesn't need scaling\n",
        "\n",
        "        # ゼロ近傍の予測は方向を判断しにくいため、閾値を設けることも検討できますが、\n",
        "        # np.sign()は0を0として扱うため、これは通常問題になりません。\n",
        "        true_direction = np.sign(y_true_single)\n",
        "        pred_direction = np.sign(y_pred_single)\n",
        "\n",
        "        # 両方が0の場合も一致とみなされることに注意。\n",
        "        # 例えば、真値が0.001で予測が-0.001の場合、np.sign()は1と-1になり不一致。\n",
        "        # 真値が0で予測が0の場合、np.sign()は0と0になり一致。\n",
        "        correct_directions = np.sum(true_direction == pred_direction)\n",
        "        total_predictions = len(y_true_single)\n",
        "        accuracy = correct_directions / total_predictions * 100\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    # ホライズンラベルは1-indexedで表示\n",
        "    horizon_labels = [f'{i+1}D' for i in range(num_horizons)]\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    bars = plt.bar(horizon_labels, accuracies, color='skyblue')\n",
        "    plt.ylabel('Directional Accuracy (%)')\n",
        "    plt.xlabel('Prediction Horizon')\n",
        "    plt.title(f'Directional Accuracy Across Prediction Horizons ({set_name} Set)')\n",
        "    plt.ylim(0, 100) # 0%から100%の範囲\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 1, round(yval, 2), ha='center', va='bottom') # accuracy値を表示\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 訓練・検証・テストセットの結果をプロット\n",
        "# global TARGET_SCALING_FACTOR_GLOBAL を使用するか、preprocess_dataから返された値を使用\n",
        "# ここでは例として TARGET_SCALING_FACTOR_GLOBAL を直接渡しています。\n",
        "\n",
        "plot_results_multi_horizon(history, y_train_seq, train_preds, dates_train, 'Train',\n",
        "                           pred_horizon_to_plot=[1, 5, 20], # 例として20日後までプロット\n",
        "                           target_scaling_factor=TARGET_SCALING_FACTOR_GLOBAL)\n",
        "plot_results_multi_horizon(history, y_valid_seq, valid_preds, dates_valid, 'Validation',\n",
        "                           pred_horizon_to_plot=[1, 5, 20],\n",
        "                           target_scaling_factor=TARGET_SCALING_FACTOR_GLOBAL)\n",
        "plot_results_multi_horizon(history, y_test_seq, test_preds, dates_test, 'Test',\n",
        "                           pred_horizon_to_plot=[1, 5, 20],\n",
        "                           target_scaling_factor=TARGET_SCALING_FACTOR_GLOBAL)\n",
        "\n",
        "# 方向精度を全てのホライズンでプロット\n",
        "plot_direction_accuracy_multi_horizon(y_train_seq, train_preds, 'Train',\n",
        "                                     target_scaling_factor=TARGET_SCALING_FACTOR_GLOBAL)\n",
        "plot_direction_accuracy_multi_horizon(y_valid_seq, valid_preds, 'Validation',\n",
        "                                     target_scaling_factor=TARGET_SCALING_FACTOR_GLOBAL)\n",
        "plot_direction_accuracy_multi_horizon(y_test_seq, test_preds, 'Test',\n",
        "                                     target_scaling_factor=TARGET_SCALING_FACTOR_GLOBAL)"
      ],
      "metadata": {
        "id": "_db5xgfAu0e9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}